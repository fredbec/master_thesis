In recent years evidence in epidemiology - as well other fields - has accumulated that in order to obtain accurate and well calibrated forecasts of targets of interest, such as case numbers of a disease in a given week, it is often advisable to not rely solely on single model outputs, but to rather consider an aggregate of forecasts made by a group of models, widely referred to as ensemble forecasts \todos{(give some cites)}. \\
The recent COVID-19 epidemic has turned out to be no exception in this regard. In order to obtain an accurate picture of current disease dynamics for decision makers and following similar efforts in the US, in March 2021 the \ac{ecdc} has instigated the European Forecast Hub, collating weekly real-time distributional forecasts for short-term incidence COVID-19 cases and deaths from independent modeling teams \cite{noauthor_european_2021}. It was found that, in general, ensemble forecasts that aggregate all single model outputs into a single common forecast showed more consistent performance than any single model for both case and death incidence forecasts \cite{sherratt_draft_nodate}. \todos{(citecitecitesomemore)}. \\
However, while evidence for the advantages of employing an ensemble strategy is ubiquitous, %it is without a doubt in general preferable to use ensemble forecasts \todos{(why)},
 a question that remains %and provides difficult to be conclusively answered 
is which type of model or ensemble structure gives an edge over others, as well as exploring the interaction between ensemble and model structure. Hence, the question is whether we can identify certain individual modeling or ensemble strategies that consistently perform better than others within the European Forecast Hub. Or alternatively - in lieu of succeeding to establish universal dominance statements - whether it is possible to identify certain situations in which some models or ensemble compositions have an edge over others. %and if it possible to leverage this knowledge for the composition of the ensemble forecast. 
In this thesis, the focus will lie on three dimensions of this issue:\\
%Or, , if it is perhaps possible to identify certain characteristics of e.g. current disease dynamics that warrant using certain models or ensemble types over others. 
First, for the goal of eliciting accurate short-term forecasts, % there is ongoing debate in the modeling community 
it is an ongoing topic of research whether models that are to some extent explicitly epidemiological in nature, that is, seeking to model transmission dynamics in a population, are to be preferred over models that solely rely on past information of the target time series and are thus agnostic to the underlying transmission dynamics \citep{funk_short-term_nodate}.  %there is no definitive answer to this, with 
Different diseases and epidemics warrant/require varying approaches in modeling strategy and we thus aim to investigate 
%with different diseases and epidemics or even the same disease but different countries requiring different philosophies in modeling. 
whether, for the European Forecast Hub, definitive or (more likely) situation-dependent rankings can be established. For example, \cite{bracher_pre-registered_2021} identified that statistical models, which rely on past time series dynamics for prediction, are consequently slow to respond to changes in trends, raising the question whether compartmental models fare better in this regard. These types of results can then potentially also be leveraged for forecast composition - in fact, \cite{taylor_combining_2021} conjecture that during low-incidence periods, compartmental models should perform better than statistical ones and that ensembles consisting purely of these models should consequently exhibit better performance. Lastly, one must not forget that there might be different requirements and goals for forecasts depending on who is using them and under which circumstances - these differing goals can be captured with the choice of  a corresponding scoring rule. To this end, it is conceivable that, for instance, one model type fares better with regard to point accuracy, while another might exhibit better coverage - for example, it could be evident that one model type exhibits more overconfidence in certain situations. We will thus investigate how the preferred model type varies with the choice of scoring rule.\\
%This translates to the question of as a function of the employed scoring rule, which we will also investigate in this section.\\
%A further dimension this can be explored in is the choice of scoring function used. \\
Another central question in past and current analysis of both European and US COVID Hub data has been whether ensembling procedures should discriminate between their potential member models based on merit. That is, if the ensemble should assign higher weights to models that, in terms of the scoring metric of interest, have performed better in the past than other models, as opposed to applying equal weighting for all models. Results on this procedure of performance-based weighting have been somewhat mixed. For predicting number of deaths in the US forecast hub data, \cite{taylor_combining_2021} find that, especially for states that exhibit high mortality, performance-based weighting leads to higher prediction accuracy. \todos{(Also include paper by Brooks here, which found no advantage.)} Conversely, in the European Forecast Hub, \cite{sherratt_draft_nodate} find no significant improvement of weighted methods in comparison with unweighted and, similarly, \cite{bracher_pre-registered_2021} also find no systematic benefits of the weighted approach for data from Germany and Poland. \cite{taylor_combining_2021} identify a possible reason for the shortcomings of weighted approaches: they require comparable records of historical accuracy and are thus challenging to implement in datasets where model availability fluctuates - this is presumably especially an issue for the European Forecast Hub, where it is common to observe large participation gaps for models. \\ 
Lacking evidence for an alternative superior ensembling technique, the European Forecast Hub has thus relied on the unweighted mean ensemble, which was then superseded by the unweighted median ensemble due to its higher resistance to outliers \citep{sherratt_draft_nodate}. Consequently, the question arises whether it is perhaps possible %(through the investigation of ensemble behavior) 
to establish some guidelines - for example pertaining to situational circumstances, model structure or relation between models - in a data setting where it is not feasible/beneficial to rely on hard and fast mathematical rules via weighted ensembles. %To this end, we want to investi. 
The hope is that through the investigation of ensemble behavior in response to these issues, we can establish some heuristics for when it is useful to have certain models in an ensemble or, as a softer goal, to simply gain more insight into how ensemble performance varies in response to the aforementioned dimensions.\\ 
There are several possible dimensions to investigate with regard to this research question, with some lines of inquiry having come up before in earlier studies. For instance, a worthwhile question to investigate is whether a model can consistently be underperforming (by one or any scoring metric used) in comparison to other individual models and the ensemble, but can nevertheless provide occasional or consistent benefit when included in said ensemble. An easy example would be a model that consistently underpredicts a target: this is of course in and of itself undesirable, but could provide great benefit to an ensemble that has a tendency to overpredict the target - a similar case was identified in \cite{bosse_comparing_2021}, and it would be interesting to investigate whether such models can be found in the existing pool of the Hub models. Along these lines, there is also the question whether the choice of adding an additional model is somehow dependent on the summary function used to generate the ensemble: in the case of the models investigated by \cite{bosse_comparing_2021}, it turned out to be somehow ``safer'' to add models in a median than a mean ensemble, presumably as it is more resistant to outliers. \\
With regard to the aforementioned wide success of ensembles in the forecasting realm, a notion that seeks to explain this success is that averaging over a number of separate models both acts as a mitigator for individual model bias and reduces overall performance variation \todos{(do the cites)}. It is thus conceivable that including models that are too similar and hence somehow make ``the same type of mistake'' (be it directional, or in relation to over-/ or underconfidence) could, in a sense, hijack/overpower/overtake/skew the ensemble and thus be detrimental to its performance. In turn, this would mean that establishing a notion of ``too big'' \todos{(find more succinct word)} model similarity and consequently culling models based on this notion could be beneficial. To this end, we use the Wasserstein 2-metric/Cramer distance, as applied to a discrete set of quantiles and investigate whether excluding single or multiple models that form a sort of ``model cluster'' improves ensemble performance. \\
Another question we'd like to investigate with regard to ensemble composition is the consistency and variation of forecast performance in relation to the number of its member models. As we believe that including additional models lowers the variation of the ensemble and thus improves its performance, the expectation here is ``more is always better'' - nevertheless, more knowledge on how exactly ensemble performance relates to ensemble size could be very valuable, especially in situations where resources are limited and it's not immediately clear whether investing into additional models would be rewarding. To this end, we will randomly sample all sets of member models in the Hub, as an answer to the question of ``what would have been if we'd have less models?''. Furthermore, we consider whether ensemble performance on average declined in weeks where not a lot of forecasts were available.\\
%Thus, the question arises whether it is perhaps possible to utilize unweighted approaches, but to tweak their model composition in such a way that makes use of guidelines, e.g. pertaining to situational circumstances or model structure, rather than hard and fast mathematical rules (i.e. weighting based on past performance scores). %We thus want to investigate . 
As already mentioned, the entire procedure should, to some extent, be regarded more investigatively/inquisitively and with the aim to establish heuristics/soft guidelines rather than hard and fast rules. By nature, the methods described here have a certain ad-hoc character, in a way that having a mathematically formulated rule that ``simply'' weights by past performance is not. It is possible that no exact guidelines emerge from the analysis, or that emerging results will be very specific to the data at hand and not necessarily generalizable. Nevertheless, we still deem there to be value in this type of analysis, as it can lead to greater understanding of ensemble behavior - performing such an inquisitive deep dive can be regarded as the novel contribution of this thesis.\\
There are several dimensions one can investigate here: low-incidence periods, periods of exponential growth, periods where not a lot of models are available, model similarity.
% We already mentioned the possibility of only utilizing a certain model type in some preiods,...Theoretically, there are some ponderings on this: for instance, \cite{taylor_combining_2021} conjecture that during low-incidence periods, compartmental models (i.e. explicitly epidemiological models) should perform better than statistical ones. 
%Another question is whether "bad" models can benefit ensembles. In their study, they identified a model \cite{bosse_comparing_2021} \\ 
Finally \todos{(note: and if there is time)}, we want to consider whether tweaking the method of ensemble building might lead to an increase in performance. The aforementioned/currently used methods treat each quantile forecast as separate and thus build a common ensemble forecast by applying some sort of summary function (usually mean or median) to each separate quantile set. \footnote{in the case of the median, as identified by \cite{bracher_pre-registered_2021}), this might lead to not so well formed distributions.} However, a potentially sensible/worthwhile/viable alternative approach could be first building a common probability distribution from the set of available forecasts, then taking the quantiles from this aggregate/mixture distribution. To this end, we consider imputing a \ac{cdf} for each forecast separately, then taking the ensemble's quantiles from the aggregate/mixture \ac{cdf}. Put succinctly, we want to investigate whether aggregating the forecasts in \ac{cdf} rather than quantile space could provide a benefit. \\
%The results will also depend on the goal of the forecaster, whether they, for instance, seek to obtain a point forecast that is as accurate as possible -, these goals and considerations are reflected in the scoring metric used, with different scoring metrics giving preference to different models.   During , do some models exhibit more overconfidence? \\
In a nutshell, the goal is of this thesis is to investigate ensemble behavior as it relates to its member models and the current epidemic circumstances, with the hope of potentially deriving some heuristic guidelines for ensemble composition from the findings.
%in a case where patchy past data does not allow for hard and fast mathematical rules.
%situation-dependent guidelines can be constructed for  
\newpage
%Now, compartmental models are wihtout a doubt preferable, as statistical models are unable to project beyond a few weeks