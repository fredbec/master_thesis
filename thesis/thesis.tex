\input{preambel.tex}
\input{acronyms.tex}

\begin{document}



%----------------------------------------------------------------------------------------
%	HEADING TITLE AND OTHER STUFF
%----------------------------------------------------------------------------------------

%\textsc{Georg-August Universität Göttingen}\\[1.5cm] % Name of your university/college
\input{titlepage.tex}

% TOC
\tableofcontents

\pagenumbering{Roman}
\clearpage

% List of...
\listoffigures
\clearpage


% Acronyms
\printacronyms
\clearpage


\pagenumbering{arabic}

%----------------------------------------------------------------------------------------
%	ACTUAL TEXT BEGINS
%----------------------------------------------------------------------------------------


\normalsize
\vspace{2cm}
\section{Introduction}
\input{introduction.tex}
We want to emphasize once again that our goal is not necessarily to find a new method that should replace the use of the current (median) ensemble in the European Forecast Hub. Even in the US forecast hub, the median is still the ensemble of choice, as it is simple and robust \citep{ray_ensemble_2020}. Occam's Razor. Rather, our study should be seen more inquisitively - even with hindsight, is it possible to beat the median ensemble? Thus, to put more on top (turning the table by 180 degrees), our entire study could thus be seen as one giant effort to support the continued use of the simple median ensemble. We love to see it. Ensembles are still largely misunderstood. Mainly, we want to investigate the question how ensemble behavior responds to adding more member models, adding bad/good performers or adding models that are very close/more distant to the current ensemble.
\section{Forecasting and Ensembles}
\subsection{What is forecasting?- the forecasting paradigm}
A forecast is an explicit quantitative prediction of the probability of a future event, be it binary (e.g. the probability of event $X$ happening by date $Y$), categorial or (quasi-)continuous (e.g., in the context of COVID-19, the incidence of cases or deaths at date $Y$) \cite{reich_collaborative_2022}. However, since this practice relies on the relevant circumstances affecting the forecast quantity to be somewhat constant, the time horizon in forecasting is generally limited. In particular, in the case of the COVID-19 epidemic, uncertainties about changes in the epidemic process, e.g. the potential emergence of new variant, or in human behavior through e.g. alterations of governmental regulations, heavily limit the viable forecast horizon and hence, horizons were generally limited to 4 weeks in most forecasting efforts \cite{reich_collaborative_2022}. In fact, in subsequent analyses of COVID-19 forecasts, it was generally found that forecast performance for incidence cases usually sharply drop after only two to three weeks \todos{(cite)}.\\
We briefly also mention the practice of scenario modeling, which, in contrast to forecasting, seeks to elicit future trajectories of the quantity of interest under a pre-defined set of settings for the relevant circumstances affecting said quantity \cite{reich_collaborative_2022}. In the context of COVID-19, these settings can be related to, for instance, specific policy interventions, levels of vaccine availability or efficacy, emergence of new variants of the virus, or any combinations of these. Scenario modeling can be used to inform decision makers that seek to evaluate the plausible effects of potential strategies, e.g. specific disease control measures, under a variety of circumstances \cite{reich_collaborative_2022}.\\ 
In general, forecasts can directly be evaluated against the truth data that realized, while scenario models are harder to evaluate, as it is generally unlikely that the settings characterizing/underlying the scenario actually realized in the exact way they were defined \cite{reich_collaborative_2022}.
"Forecasting is the ability to predict what will happen in the future on the basis of analysis of past and current data" \cite{moran_epidemic_2016}.
Sharpness subject to calibration.\cite{ray_ensemble_2020}.
\begin{figure}
\centering
\includegraphics[width = 0.95\textwidth]{../plots/sharp_calib.pdf}
\end{figure}

\subsection{Ensemble Forecasts}
They have a long tradition in weather forecasting, where they show consistently improving performance over single models. \\
An ensemble aggregates models, thereby unifying their respective knowledge/signals into a single forecast.\\
Ray2020: "Multiple studies of epidemic forecasting have shown that ensemble forecasts, which incorporate multiple model predictions into a combined forecast, consistently perform well and often outperform most if not all individual models (Viboud et al. 2018; Johansson et al. 2019; McGowan et al. 2019; Reich, Brooks, et al. 2019)." Conversely, \cite{bracher_pre-registered_2021} did not find that the ensemble outperformed single models.
\subsection{US forecast hub}
Numerous studies have come out for the US forecast hub. Taylor and Taylor, Ray 2020. \cite{ray_ensemble_2020}.
\subsection{Epidemic forecasting}
There are a few characteristics that can make forecasting epidemics particularly challenging: firstly, epidemic models seek to model transmission dynamics, which are not constant and can change during the course of an epidemic, namely (but not exclusively) due to human behavior changes and/or changes in the relevant pathogen \citep{moran_epidemic_2016}. Furthermore, data sources come with their challenges: While it is clear that timeliness is of critical importance for disease surveillance, data are oftentimes not readily available in real-time, and are sometimes made available only months after initial recording \cite{jajosky_evaluation_2004} \todos{(more directly cited from Moran)}. Different data streams, all with own challenges \cite{moran_epidemic_2016}. Briefly describe truth data collection in Covid-19 epidemic. One could argue that in the recent Covid-19 epidemic, there were major improvements in the timeliness of reporting, yet data continued to see retroactive revisions, sometimes substantially \todos{(cite)}. 
\subsubsection{Model types in epidemic forecasting}
Various different models and methodologies can be used to issue forecasts, and epidemic forecasting is no exception in this regard. The most common models used in epidemiology and within the data used for this thesis can broadly be sorted into categories, which we will briefly introduce and discuss in the following. For sorting models, we consulted the metadata files that teams submitted into the Hub, as well as any publications dealing with the relevant model - furthermore, we closely aligned to \cite{bracher_pre-registered_2021} who used the same categories when sorting their models. We will give some examples of models that fit into these categories from the Hub data\footnote{For a full list of models from the European COVID-19 forecast hub, consult https://github.com/covid19-forecast-hub-europe/covid19-forecast-hub-europe/tree/main/data-processed}\\
Compartmental - alternatively named mechanistic - models are among the most widely used models in epidemiology to model infectious diseases. The term ``compartmental'' derives from the fact that these models divide the population into compartments according to their infection status with respect to the infectious disease at time $t$. In their most basic structure, these compartments are $S$, $I$ and $R$ \citep{brauer_epidemic_2012}: $S(t)$ denotes the number of individuals that are susceptible to the disease at time $t$, that is, they can potentially be infected with the disease. $I(t)$ denotes the number of infected individuals, which are assumed to be able to spread the disease when in contact with individuals from compartment $S$. Finally, $R(t)$ denote the number of individuals that, after being infected, have been removed from the process at time $t$. This compartment thus contains individuals that are either isolated, recovered/immunized without possibility of reinfection, or dead as a consequence of the disease. Additional characteristics of the epidemiological process of the disease, e.g. the availability of a vaccine, can be modeled via extra compartments. \todos{(this is a weird sentence)}. Commonly added extra compartments are $E$ for exposed but not yet infectious individuals, giving an SEIR model, or an additional $S$ compartment to model non-permanent immunity from the disease, resulting in an SIRS model \citep{brauer_epidemic_2012}. \\
Given some regularity assumptions and a set of parameters governing the transmission process, flow of individuals between the compartments is modeled via a set of differential equations, from which key parameters again derive - most notable among them is the reproduction number $R_t$, which denotes the average number of susceptible individuals that an individual from compartment $I$ infects at time $t$. However, the reproduction number is not an explicitly modeled parameter in these frameworks.\\
Furthermore, there exist models that are built on epidemiological information but do not have an explicit compartmental framework - these models we sort into the \textit{semi-mechanistic} category. This category most notably includes models that model cases or deaths via growth rate approaches, but also contains models that are based on other epidemiological principles, for example the renewal equation. For the former, the general strategy is to estimate a time-varying growth rate, which is then mapped to predicted infections.\footnote{the reproduction number $R_t$ and the growth rate $r_t$ are separate, but related quantities} Deaths are then usually modeled as a fraction of infections. The texttt{LANL-GrowthRate} model estimates. The \texttt{EpiNow2} model models the reproduction number via a non-stationary Gaussian Process,  while the . \\
Another strategy for forecasting in epidemiology are statistical models. While the aforementioned methods of course also utilize statistical methods in estimating key parameters, this category refers to models that rely solely on statistical methods (\todos{noch nicht gut genug abgegrenzt}) and are thus agnostic to the underlying transmission dynamics - put succinctly, \cite{holmdahl_wrong_2020} refer to this strategy as ``crunch[ing]'' epidemiological data from the past [...] and project[ing] cases into the future''. For instance, these can be ARIMA models, such as the \texttt{MUNI-ARIMA} model in the Hub.\\
Overall, these model types all have their differing advantages and disadvantages. Due to the fact that they rely solely on past time series data to make predictions, statistical models' predictions are usually only suited for forecasting (recall: predicting for short time horizons under the current status quo), while mechanistic models are also suited for longer term planning via scenario modeling \cite{reich_collaborative_2022}. Furthermore, as mechanistic models explicitly attempt to model underlying transmission dynamics, they can generate insights of the process that purely statistical models cannot \cite{james_use_2021} \todos{(check if this was actually said here)}. On the flip side, this means that model accuracy is also limited by knowledge about the virus \cite{holmdahl_wrong_2020}.
Overall, it can be said that there is no consensus in the epidemic modeling community on which of these model types performs best \cite{moran_epidemic_2016}.\\
Of course, this categorization of modeling strategies is not entirely clear-cut and also not  exhaustive: furthermore, there exist agent-based approaches, which model epidemiological processes by simulating behavior of each individual - \cite{zelner_accounting_2021} state that these can provide useful insight but can be very difficult to fit to data. In fact, within the subset of the Hub data used in the thesis, only two models are agent-based and only for Poland. These only predict for Poland, presumably as they require a lot of tuning. \\
Moreover, there exist various efforts in the community to establish approaches that are based on human judgment, by either aggregating forecasts from experts or by random peeps. \cite{bosse_comparing_2021} saw promising results from a crowd forecast, which outperformed ensemble methods for forecasting cases (but not deaths) - critically, forecasts based on human judgment are thought to be better able to anticipate changes in trends, as they are, for instance, able to incorporate knowledge about changes in policy in a way more rigid modeling strategies are not \cite{bracher_evaluating_2021}. This is especially valuable due to the fact that other models have almost universally been found to be quite bad at predicting changes in trends \todos{(cite some Reichlab paper that says this, as well as Bracher DE-PL)}. One could thus conceive an idea whereby one combines expert judgment and modeling output in a way that best takes advantage of their respective strengths: use human expert forecasts as an ``alarm bell'' for trend changes and thereby rely more on them in such times, resort to ``normal models'' when alarm bell is not sounded. This is also valuable due to the fact that crowd forecasts have been found to be overly confident \cite{bosse_comparing_2021}. Namely, the tradition of expert forecasts has some previous tradition in the field of Economics/Econometrics: the Survey of Professional Forecasters often serves as a powerful benchmark in the field \todos{(cite)}.\\
Since the availability of agent-based models and expert judgment models is very limited, we mainly focus on the models in the categories mechanistic, semi-mechanistic and statistical. \\
\section{Scoring}
Suppose that $y$ is the realisation of a random variable under the true data-generating distribution $G$. The forecasting problem is defined by trying to issue a predictive probability distribution $F$ for the future realisation of this random variable. Further, denote $s(F,G)$ for the expectation of $\text{E}[s(F,y)]$. We then say that scoring rule $s$ is \textit{proper}, if 
\[s(G,G) \leq s(F,G).\]
Put into words, this means that the scoring rule is minimized if the true data-generating distribution is issued as the forecast distribution. Likewise, the scoring rule $s$ is \textit{strictly proper}, if 
\[s(G,G) < s(F,G).\] 
A (strictly) proper scoring rule thus incentivizes the forecaster to issue his or her true belief for the predictive probability distribution.\\
This notion of the propriety of scoring rules originated with \todos{Winkler and Murphy (1968)} and its importance in the forecasting world (hmpf) cannot be overstated - if a scoring rule for distributional forecasts is not proper, it could, for instance, incentivize a forecaster to report a more confident estimate than he or she actually believes in \todos{Thorarinsdottir 2013}. \\
\subsection{Scoring rules}
We focus on evaluating model performance via two central principles, that is, accuracy (coverage/calibration) and overall predictive performance (weighted interval score). \citep{sherratt_predictive_2022}.
\subsubsection{PIT}
\subsubsection{Coverage}
Prediction interval coverage measures the proportion of values that fell into a predictive interval of a given level and thus reflects how well a model was able to characterize uncertainty over time.\citep{cramer_evaluation_2022} \todos{(from SI, cite something better. Scoringutils paper is also a good reference)}. It measures probabilistic calibration \citep{bosse_evaluating_2022}.
\subsubsection{Weighted Interval Score} \label{ssub:weighted_interval_score}
Here, we introduce the \ac{wis}, which is the main scoring rule used within this thesis \cite{bracher_evaluating_2021}. It is designed for use on probabilistic forecasts \cite{european_covid-19_forecast_hub_european_2021} $F$ that are issued as a set of discrete central prediction intervals, each with nominal coverage level $\alpha$ - or, put differently, as a set of symmetric predictive quantiles $q$ which directly translate to central prediction intervals. \\
Each central prediction interval can be scored via the interval score \citep{gneiting_strictly_2007}
\begin{equation}
IS_{\alpha}(F, y) = (u-l) + \frac{2}{\alpha}(l - y)\mathbb{1}(y < l) + \frac{2}{\alpha}(y - u)\mathbb{1}(y > u),
\end{equation}
where $\mathbb{1}$ is the indicator function, returning 1 if the condition inside the parentheses is fulfilled and 0 otherwise. The three summands each have an intuitive interpretation. The second and third summands express under- and over-prediction, respectively. They assign a penalty if the true observed quantity $y$ falls below (above) the lower (upper) endpoint $l$ ($u$) of the prediction interval. The first $(u-l)$ expresses the width of the central prediction interval and thus the sharpness of the predictive distribution $F$ - if this term didn't exist, it would make sense to simply issue very large prediction intervals that are highly likely to contain the true observation $y$. These penalties are furthermore scaled by the nominal coverage level: a smaller $\alpha$, which corresponds to a higher nominal coverage rate, induces a higher penalty if $y$ does fall outside one of the endpoints. \\
\cite{bracher_evaluating_2021} extend this score for use on a predictive distribution $F$ that consists of a set of such intervals, each with unique coverage level $\alpha$. The set of interval scores is gathered and aggregated into the weighted interval score
\begin{equation}
WIS_{\alpha_{0:K}}(F,y) = \frac{1}{K + 1/2}\left(w_{0}|y-m| + \sum_{k=1}^{K}\left(w_k IS_{\alpha_{k}}(F, y)\right)\right),
\end{equation}
where usually the quantile weights are set to $w_k = \frac{\alpha_{k}}{2}$, and the median weight to $w_{0} = \frac{1}{2}$.\\
It can be shown that the \ac{wis} is an approximation of the \ac{crps}, a well-known scoring function that measures the distance between the predictive and true distribution 
\begin{equation}
CRPS(F, x) = \int_{-\infty}^{\infty} \left(F(y) - \mathbb{1}(y \geq x) \right)^2dy.
\end{equation}
All in all, the \ac{wis} is a parsimonious way to score forecasts that come in the shape of a set of discrete intervals \citep{sherratt_predictive_2022}.
An important "feature" of the WIS is that it is not standardized and thus scales with the data. This can be easily seen in equation ?, as the absolute differences of the observed value and the predicted quantile directly enter into the score. Thus, scores will naturally increase if the target to be predicted increases. This makes forecast comparisons a bit difficult, which leads us to the next point.
\subsection{Pairwise comparisons}
One issue that often arises when aiming to compare different forecasting models is a potentially non-overlapping base of targets the models predicted for, as some scoring rules are not normalized and thus scale with the data. For instance, if models were compared via average WIS, one model might look better than another if it only predicted in periods that saw low incidence or were otherwise comparatively "easy" to forecast. This would thus disincentivize forecasters to predict in periods that they perceive to be more challenging - this is especially undesirable because these periods (e.g. exponential growth, high level of infections) are often of special interest to decision makers \todos{(cite something)}.\\
One can address this by computing a relative score that is based on employing pairwise comparisons, as developed in \cite{cramer_evaluation_2022}. For a pair of models denoted $l$ and $m$, first a measure of relative skill is computed
\[
\theta_{l,m} = \frac{\bar{s}_{l}}{\bar{s}_{m}},
\]
where $\bar{s}_{l}$ and $\bar{s}_{m}$ denote the average scores the models achieved on the targets both models predicted on - this is usually chosen to be the WIS. For each model, the geometric mean of these relative scores is then computed as
\[
\theta_{l\cdot} = \left(\prod_{m = 1}^{M}\theta_{l,m} \right)^{\frac{1}{M}},
\]to obtain a relative score of model $l$ with respect to all other available models. It can thus be interpreted as a performance measure of model $l$ with respect to a model with ``average'' performance. If interest lies in a direct pairwise comparison with a specific model $m$, one can instead consider the ratio of these relative scores
\[
\phi_{l,m} = \frac{\theta_{l\cdot}}{\theta_{m\cdot}}.
\]
Calculating this ratio for all model pairs that are of interest results in a ``pairwise tournament'' for all models in the set - this approach is implemented in the \textbf{scoringutils} package \citep{bosse_epiforecastsscoringutils_2022}. For negatively oriented scoring rules, the ratio will be smaller than 1 if model $l$ outperformed model $m$ on their set of shared targets and larger than 1 if it did not. Note that this mode of pairwise comparison still requires the assumption that it is equally hard to perform relatively well to other models at all forecast dates and locations \citep{cramer_evaluation_2022}.\\
If one is interested in concisely summarizing the skill of single models rather than performing comparisons between all pairs of models, one can choose a baseline model's $B$ relative score $\theta_{B\cdot}$ as the denominator, which for the WIS results in the measure that is commonly referred to as "relative WIS". Analogously to above, a ratio below 1 corresponds to a model overall outperforming the baseline model, while a score above 1 means that the model did not succeed in clearing baseline performance.\\
Alternatively, one can summarise a model group's scores, to compare to another model group.
\subsection{general}
WIS assesses sharpness, while coverage assesses calibration. (no not true)\\
Scoring rule is not a meaningless choice: as will be demonstrated in later sections, different scoring rules induce different rankings. It all depends on what the goal is.
\section{Data}
The data used in this thesis stem from the European COVID-19 Forecast Hub (thereafter referred to as the ``Hub'' or ``European Hub'', for the sake of brevity), which was instigated by the \ac{ecdc} in 2021 to collate forecasts for Covid-19 incidence cases and deaths from independent modeling teams across Europe \citep{european_covid-19_forecast_hub_european_2021}. It was modeled after a similar previous effort in the United States, the United States COVID-19 Forecast Hub (thereafter referred to as the ``US Hub'') \citep{cramer_united_2021}. Furthermore, the preceding German-Polish forecasting hub was largely synchronized with the European Hub \citep{bracher_german_2020}. The Hub's primary goal is to "provide reliable information about the near-term epidemiology of the COVID-19 pandemic to the research and policy communities and the general public" \citep{sherratt_predictive_2022}. In general, a modeling hub is a coordinated effort, in which one or more common prediction targets, as well as a common format for prediction, are agreed upon / implemented. This serves the purpose of facilitating model evaluation and development by making model predictions comparable, as well as making predictions suitable for aggregation, that is, for ensemble predictions. \\
\begin{figure}
\includegraphics[width = \textwidth]{plot_placeholder/visualize_data.png}
\end{figure}

The "hub" format has some precedence both in the realm of climatology as well as in epidemiology, for example in forecasting influenza in the United States \cite{reich_collaborative_2019} as well as dengue fever in ... \cite{johansson_open_2019}. For these seasonal diseases, prediction targets were total number of cases in a season or the height of the peak, while in the case of Covid-19 and the European forecast(ing) hub, the common prediction target are weekly incidence Covid-19 case and death counts in 32 \todos{(check)} European countries, later also hospitalization rates. Forecasts are issued in a probabilistic manner, namely as a set of 23 quantiles of the predictive distribution, at non-equally-spaced levels between 0.01 and 0.99 (namely $\tau = 0.01, 0.025, 0.05, 0.1, 0.15, ..., 0.85, 0.9, 0.95, 0.975, 0.99$).
To be included in the Hub's ensemble and thus in this analysis, models had to provide a full set of 23 quantiles for all four horizons.\\
Include example plots of individual and ensemble predictions.\\
\begin{figure}
\includegraphics[width = \textwidth]{../plots/trajectories.pdf}
\caption{Plot showing the trajectories of the different time series. Note that the time series for Cases were normalized by the respective location's population for readability. Time series for deaths are direct incidence counts.}
\label{fig:trajectories}
\end{figure}
Incident deaths are inferred via cumulative.
Talk about how the ``ensemble is best'' paradigm has also held here, with citations to both Eu and Us FCH.\\
Talk about truth data source (JHU), and potential data issues.("We derive incident data by taking weekly differences of the cumulative data".).\\
"With the complete dataset for the latest forecasting week available each Sunday, teams typically
submitted forecasts to the hub on Monday. We implemented an automated validation
programme to check that each new forecast conformed to standardised formatting. The
validation step ensured a monotonic increase of predictions with each increasing quantile,
integer-valued counts of predicted cases, as well as consistent date and location definitions." \citep{sherratt_draft_nodate}.
"Forecast horizons should use the Epidemiological Week (EW) format, defined by the US CDC. Each week starts on Sunday and ends on Saturday. "
Talk about issues of model availability.\\
\begin{figure}
\includegraphics[width = \textwidth]{../plots/availability_indivi.pdf}
\caption{Plot showing the overall availability of each model submitted to the European COVID-19 Forecast Hub across the five locations considered in this study. It can be seen that there is large variability in the availability of each model. Some models are available during most of the study period, while others only submit forecasts for a small section of the considered period. It can also be seen that there are some models that submit predictions for several or most locations, while others only predict for one specific location.}
\end{figure}
The Hub also includes a ``naive'' baseline model, which is the same as the one that is used in the US Covid-19 Forecast Hub \todos{(cite)}. For each forecast date, its forecast for median incidence is equal to the last value for incidence Cases/Deaths that was observed in the most recent week. For uncertainty around the median, the other predictive quantiles are taken from Monte Carlo approximations of the empirical distribution function that is induced by the first differences observed in the respective time series \citep{cramer_evaluation_2022}. Including a baseline model serves the purpose of providing a sort of ``minimum'' performance that models should be able to clear. Reporting that a model performs better than the baseline thus gives validity to the performance of that model. It's a reference model that all models can be compared against. In essence, this model can thus be seen as a martingale/random walk model\\
For the subset used in this study, we decided ahead of our study to only use a subset of the available countries in the set. This is simply due to the fact that most of the analyses we aimed to perform required a sufficient model base as support. Furthermore, we decided to only use data up until Mid-December of 2021. This is due to the fact that model base dropped around Christmas,. Moreover, beginning with the new year 2022, Omicron\textsuperscript{TM} entered the chat and in some countries, testing criteria changed. We thought that this could make the time periods less comparable.\\
\subsection{Hub Data}
Issued as 23 quantiles. Mostly use \ac{wis}, which corresponds to giving slightly larger weight to intervals with large nominal coverage, as compared to \ac{crps} \citep{bracher_evaluating_2021}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introspective stuff}
\subsection{Model Types}
First and foremost, we had to categorize the models in the data in a meaningful manner. We decided on a categorization. To this end, we . Teams that mentioned an explicit compartmental structure of SIR or related (for instance SEIR, SECIR) type we categorized as "mechanistic".\\
\citep{bracher_evaluating_2021} state that they did not find any ``striking patterns'' between model types in their analysis, but also acknowledge that this might be due to the relatively short study period they considered (12 weeks). A question that thus naturally arises is, whether given a longer study period, patterns can be found.\\
\begin{figure}
\centering
\includegraphics[width = 0.9\textwidth]{../plots/pw_comp_model_type_with_agent_avail0.pdf}
\caption{Pairwise comparison of model types. Plot was produced with package \texttt{scoringutils}}
\label{fig:pw_comp_modeltypes}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%General intro to evaluation method/data base%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Typically, the recommendation is to only compare models which are available for at least $50\% $ of the time period under study, to avoid comparison of forecasts that don't have any overlap \citep{bosse_epiforecastsscoringutils_2022}. Since we are however comparing forecasts at the level of model types rather than single models and there does not exist a pair of model types without overlap in the relevant time period, this advice does not necessarily apply here. One could still make the argument that one should not exclude models that only contribute for a very small portion of the study period, so as not to have models that more likely produce outlying forecasts\footnote{One could think that models that only have e.g. $10\%$ availability either dropped out due to not performing well and/or teams might have not been invested enough in the project to update and keep tuning their model, both of which might give low-quality predictions that are not necessarily representative of the respective model type.} influence the results too much - however, we found that the results were not at all sensitive to the exact choice of availability threshold chosen for the models, so we decided not to exclude any models here.\\ 
Nevertheless, there remains the issue of the more obscure model types: as previously stated, the two agent-based models in the set only submitted forecasts for Poland, while the expert-judgment based models dropped out of the Hub during the late summer period of 2021. We report their results the highest level of aggregation (averaged results across all locations and forecast dates), but will mostly leave them out in the subsequent analyses, for conciseness and as we think that dwelling on their performance too much might give non-generalizable results. Note that the value of the mean score ratio between two model types in the pairwise comparisons is independent of other models in the set, so the mean score ratios of the other pairs are unaffected by this choice.\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Discuss pairwise comparisons%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The results of the pairwise comparison, where for each model type, weighted interval scores are stratified by horizon and target type, but averaged across the entire study period and all locations, can be found in Figure \ref{fig:pw_comp_modeltypes}.\\
Of course, averaging across the entire study period and all locations has the issue of potentially masking some trends or effects. Furthermore, as previously stated, average WIS scores are generally vulnerable to be dominated by periods and locations with high nominal incidence levels. First of all, we thus want to evaluate whether the results presented above describe a consistent trend across the entire time period, or whether these only stem from certain phases of the trajectories. We thereby follow \cite{taylor_combining_2021} in dividing our study period into 10-week periods\footnote{since we have 47 weeks in our studied sample, we actually divide into 2 10-week and 3 9-week periods}, to investigate whether these trends hold. The results can be found in Figure \ref{fig:pw_comp_modeltypes_byperiod} For conciseness, we only report performance against the baseline.\footnote{However, as can easily be seen from equation XX, if a model type has a lower mean score ratio with respect to the baseline model than another model type, it will also outperform that model type in terms of the mean score ratio.}
First of all, we can see that performance is heavily correlated across model types: there are some periods which are harder to predict, while others . %Recall that models here are scored against a baseline model which predicts median levels to be the same in the future: this model is harder to beat in period 3, where Cases .
%%%%%%% Across horizons and locations
- we see that bad semi scores mostly seem to derive from period 5. while e.g. mechanistic models also underperform in some periods, these periods do not influence the aggregate too much.
- scores are correlated\\
- Statistical models better in period 5, for both deaths and cases. This is in line with Bracherboy.\\
In period 2, where death numbers are mostly marked by decline and low levels overall (see Figure \ref{fig:trajectories}), we observe that all model types markedly outperform the baseline model at virtually all horizons and locations. Investigating the decomposition of baseline WIS during this period, we found that the baseline model's dispersion alone sat between 0.96 (at horizon 1) and 1.46 (at horizon 2) of the \textit{overall} average WIS score of all other model types, suggesting that the baseline model issued severely underconfident forecasts during this time.\footnote{Does this mean that the baseline model was badly calibrated?} Since the baseline model's uncertainty bands are based on past differences, this suggests that these don't update quick enough. This marks the point that the choice of baseline is not entirely trivial. \\
To see that scores are relatively heavily dominated by periods and locations with high nominal incidence, consider the results from case forecasts in period 3 in Figures \ref{fig:pw_comp_modeltypes_byperiod} and \ref{fig:pw_comp_modeltypes_byloc}: all model types substantially underperform with respect to the baseline in the aggregate, but they actually outperform the baseline for Poland and the Czech Republic. Since these countries however have very low incidence numbers compared with other countries in the evaluation set during this period (see Figure \ref{fig:trajectories}), they don't influence the average score very much. Whether or not this is a desirable feature of the evaluation method is an ongoing debate and can depend on the forecasters' as well as decision makers' preferences. For instance, \cite{bracher_evaluating_2021} argue that scoring forecasts in this manner is meaningful, as a fixed relative deviation from the observed quantity can more problematic at high incidence levels rather than at very low incidence levels.  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Discuss decomposition%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
As explained in subsection \ref{ssub:weighted_interval_score}, the weighted interval score can straightforwardly be decomposed into penalties accrued from overprediction, underprediction and dispersion. A question that thus naturally arises is whether the score differences discussed previously can be explained by model types performing particularly well or not with respect to a certain component. To this end, we show the decomposed average WIS scores obtained by each model type in Figure \ref{fig:decomp_model_types}. Note that these are the raw average scores rather than scores obtained relative to the baseline model. This is possible since forecasts from all model types are available for the entire time period under study\footnote{arggh, not actually true. Statistical models missing for the first few weeks in CZ, FR, GB. Could put figure in appendix showing robustness}. We hence see that scores vary substantially more between the different forecast horizons than they do between the model types, but that we can nevertheless observe some interesting facts. 
- Lastly, we also see a general result from previous studies: cases get increasingly harder to forecast than deaths at higher forecast horizons.\\
- For both targets, forecasts from semi-mechanistic models receive higher penalties for overdispersion than the other model types.\\
- For both targets, forecasts from statistical models receive higher penalties for underprediction than the other model types.\\
- relatively bad performance for semi-mechanistic models at longer horizons (while higher overdispersion throughtout, semi-mechanistic models additionally accrue high scores for overprediction at longer forecast horizons.)\\
- for death forecasts, mechanistic models have higher overprediction component\\
- one could of course argue that 4-week ahead predictions are relatively unreliable anyway \\
Interestingly, statistical models more commonly receive penalties for underprediction. It's interesting here to mention that scoring models in terms of  underprediction is often "safer" than overprediction in terms of absolute WIS scores.\footnote{We want to pull attention to forthcoming work by Nikos Bosse, who investigates scoring forecasts in terms of relative rather than absolute errors.}  \todos{(remove this, as the thought is definitely inspired by Nikos' paper and I can't cite it?)}\\
Of course, all of this comes with the caveat. Not really a consistent pattern.
\begin{figure}
\centering
\includegraphics[width = 0.9\textwidth]{../plots/pw_comp_model_types_across_periods.pdf}
\caption{Pairwise comparison of chosen model types across time periods. The code to produce this plot was adapted from package \texttt{scoringutils}}
\label{fig:pw_comp_modeltypes_byperiod}
\end{figure}
\begin{figure}
\centering
\includegraphics[width = 0.9\textwidth]{../plots/pw_comp_model_types_across_periods_and_locs.pdf}
\caption{Pairwise comparison of chosen model types across locations. The code to produce this plot was adapted from package \texttt{scoringutils}}
\label{fig:pw_comp_modeltypes_byloc}
\end{figure}
\begin{figure}
\centering
\includegraphics[width = 0.95\textwidth]{../plots/wis_decomp_model_types.pdf}
\caption{Decomposition of the weighted interval scores of the model types}
\label{fig:decomp_model_types}
\end{figure}
\subsection{Model Similarity}
We now turn to the issue of model similarity in ensembles. As expanded upon in Section \todos{XX}, ensemble models are widely regarded to be successful due to the fact that they counteract/mitigate individual model biases and furthermore reduce variance by aggregating a number of models. Regarding the first point of mitigating bias, it is thus conceivable that ensembling approaches could be less successful if some of the included models are too similar. To illustrate this, recall the , thereby skewing 
%include plot of scaled model similarity
%plot of model performance with respect to number of models kicked out
%analyze which models are actually similar 
This notion has some mention (\todos{find better word}) in the literature. For example, in \cite{bosse_comparing_2021}, the authors mention that they purposefully did not submit one of their models for inclusion in the forecast hub's ensemble, as there was concern that it could be too similar to another model they already submitted. However, this decision based on the two models' similarity in modeling setup (shortly explain), rather than on an actual judgment of how close their predictions were. Nevertheless, they did find that both models improved the ensemble if included \todos{(find out if this was actually true)}. We now want to do a more systematic review of this concept - since we will consider more models across more countries, we hope to get a more accurate picture.\\
\begin{figure}
\centering
\includegraphics[width = 0.95\textwidth]{../plots/model_similarity.pdf}
\end{figure}
\newpage
\subsection{Adding a new model}
Note thus that the model added to the mean vs median ensemble need not be the same, although we found via monitoring that they often are.
``To enhance
interpretability of scores we mainly report WIS relative to the Hub ensemble in the main text, i.e. we divided
the average scores for a given model by the average score achieved by the Hub ensemble on the same set of
forecasts (with values >1 implying worse and values <1 implying better performance than the Hub ensemble).''\cite{bosse_comparing_2021}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%e
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ensemble Experiments}
Some of these techniques came up . We thus retroactively . In light of the fact that . One could in theory denote a holdout set, but given the fast changes in disease characteristics, the periods are not really comparable.\\ 
Note that we refrained from excessively tuning hyperparameters in the study.
We wanted to derive methods that accounted for the fact that weight estimation does not work well in low signal-to-noise ratio settings \cite{claeskens_forecast_2016}.\\
We decided to implement an ensemble that is based on the results from 1, i.e. that namely excludes semi-mechanistic models at longer horizons. Of course, we also saw that mechanistic models had rather poor performance at longer horizons, but we decided to exclude only semi-mechanistic as we otherwise wouldn't have much of an ensemble left at all. Furthermore, while the trend isn't $100\%$ clear, we mostly see that semi-mechanistic models perform either second-worst or worst. Compared with both the relative scores of the other model types, we see that semi-mechanistic models are rarely a good performer when compared to the other model types.  We of course expect this to improve performance in-sample, but further we want to investigate whether this introduces any unwanted effects, i.e. discontinuities. Namely, there are a few desirable properties of forecasts that could be in jeopardy if the set of models in the ensemble varies with the forecast horizon. It is thus conceivable that these models increase the cone of uncertainty for lower horizons, making it drop afterwards if they are then excluded. Furthermore, we also monitor the resultant coverage of the ensemble before and after, to see if anything worsened in this regard - wis might drop (not a surprise, as this is how it was selected), but that makes monitoring other scoring rules even more important. \\
For evaluation, we use a more direct relative WIS, since there are no missing forecasts in this part of the analysis.\\
\subsection{Model Types}
These compartmental models, via a set of differential equations, explicitly model how members of the population transition through the states of being susceptible, (exposed), infected, and recovered/removed \cite{taylor_combining_2021}.
\cite{taylor_combining_2021} conjecture that during periods of low incidence, mechanistic models should perform better than statistical ones. This is due to the fact that random statistical fluctuations can still occur, but statistical models might, somehow, latch on to these too eagerly and proceed to forecast exponential growth where there is none.
\subsection{Best performers}
The rationale behind this approach is that it is to a certain degree less flexible than choosing true weights, i.e. a sort of shrinkage method.
For , we also excluded those forecasts that did not realize yet, i.e. forecast date + horizon > current date.
Furthermore, in the US context, there was some success in only limiting the best performers to contribute to the ensemble.
\subsection{Weighting based on model types}
It is perhaps overeager to assume that one model type could systematically outperform another over the entire study period which, after all, comprises different countries with varying periods of infection dynamics. One could however then conjecture that in specific situations, one modeling philosophy could be better suited than another. For instance, \cite{taylor_combining_2021} hav surmised whether in periods of low infection rates, [ensembles of] compartmental models might be best suited for forecasting, while \citep{bracher_pre-registered_2021} have identified that following changes in trends, statistical models perform more poorly, likely because they first have to observe the changing trend in the data from which they extrapolate.\\
To echo this, during our investigation of model types in section ??, we also observed that , although these did not necessarily follow any strict rules as outlined above. Nevertheless, if we for instance regard Figure ??, we can most prominently see for e.g. France that model type performance varies by period, with one type dominating the other at certain times. It would thus have been preferable to use only one type of model. Thus, we want to investigate whether these relative performance differences can be picked up by an automatic weighting scheme. \\
Schreib irgendwas darüber warum vielleicht ein Modelltyp einem anderen theoretisch überlegen sein könnte in bestimmten Phasen. Statistical exponential,... This might be hard to implement explicitly, but automatic weighting might work.\\
We aimed at categorizing phases of the pandemic into some notion of (``stable'', ``upswing'', ``downswing''), but ultimately found that any such categorization would be arbitrary to a non-neglectable degree. Instead, the idea is to use recent performance of the model types to weight them differently in an ensemble. As mentioned, due to models dropping in and out of predicting, there are substantial gaps for each model and thus not yielding a consistent record of recent performance that one could use to weight on. On the level of model types, we don't have these gaps. The idea thus is to first build an ensemble for each model type in the data and then to weight these respective ensembles with respect to their recent performance. The hope is that if there are periods of the pandemic where a certain model type has an edge over others, this system will recognize this and weight the models accordingly. \\
The strategy was as follows: we first built a median ensemble based on each group of model types. We chose the median ensemble as it is more resistant to outliers, which can be more problematic in the smaller group of models that were created here.\\
We decided to use an exponential weighting scheme to assess the recent score of model types. We felt that such a weighting scheme would strike a good balance between bias and variance when estimating recent performance of a model: using only a few recent observations to build average performance gives low bias, but also increases variance due to possible random fluctuations in model performance. Exponential weighting of the scoring history gives more weight to recent scores, but also includes faraway scores to a lesser degree so as to reduce the variance of the estimation.\\
\todos{(maybe include weighting as in recent Brooks/Ray/Reich paper?)}
\begin{figure}
\centering
\includegraphics[width = 0.95\textwidth]{plot_placeholder/perform_ensemble_ovrtime.png}
\caption{From \cite{ray_ensemble_2020}. Plot different ensemble techniques over time}
\
\end{figure}
\begin{figure}
\centering
\includegraphics[width = 0.95\textwidth]{plot_placeholder/perform_ensemble_ovrtime2.png}
\caption{From \cite{ray_ensemble_2020}. Plot different ensemble techniques over time}
\
\end{figure}
\bibliography{references}
\bibliographystyle{plainnat}
\end{document}