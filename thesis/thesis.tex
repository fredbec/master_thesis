\input{preambel.tex}
\input{acronyms.tex}
\usepackage{array,multirow,graphicx}

\begin{document}



%----------------------------------------------------------------------------------------
%	HEADING TITLE AND OTHER STUFF
%----------------------------------------------------------------------------------------

%\textsc{Georg-August Universität Göttingen}\\[1.5cm] % Name of your university/college
\input{titlepage.tex}

% TOC
\tableofcontents

\pagenumbering{Roman}
\clearpage

% List of...
\listoffigures
\clearpage


% Acronyms
\printacronyms
\clearpage


\pagenumbering{arabic}

%----------------------------------------------------------------------------------------
%	ACTUAL TEXT BEGINS
%----------------------------------------------------------------------------------------

%\small
\vspace{2cm}
\section{Introduction}
\input{introduction.tex}
We want to emphasize once again that our goal is not necessarily to find a new method that should replace the use of the current (median) ensemble in the European Forecast Hub. Even in the US forecast hub, the median is still the ensemble of choice, as it is simple and robust \citep{ray_ensemble_2020}. Occam's Razor. Rather, our study should be seen more inquisitively - even with hindsight, is it possible to beat the median ensemble? Thus, to put more on top (turning the table by 180 degrees), our entire study could thus be seen as one giant effort to support the continued use of the simple median ensemble. We love to see it. Ensembles are still largely misunderstood. Mainly, we want to investigate the question how ensemble behavior responds to adding more member models, adding bad/good performers or adding models that are very close/more distant to the current ensemble.
\section{Forecasting and Ensembles}
\subsection{What is forecasting?}
We want to begin by defining the practice of forecasting, as we observe that it is  sometimes conflated with related but distinct terms, such as the more general task of prediction. \cite{moran_epidemic_2016} define forecasting as the ``ability to predict what will happen in the future on the basis of analysis of past and current data''. Where prediction modeling is thus the practice of issuing categorical or quantitative statements for any type of unseen data, forecasting explicitly refers to the case where that unseen data lies in the future, and we make predictions based on historical data.\\
More concretely, we define a forecast as an \textit{explicit quantitative prediction} of the probability of a future event, be it binary, such as the probability of event $X$ happening by date $Y$, categorical or (quasi-)continuous (e.g., in the context of this thesis, the level of incidence of COVID-19 cases at date $Y$) \citep{reich_collaborative_2022}. As such explicit and non-contingent statements about a future quantity rely on the relevant circumstances affecting said quantity to be somewhat constant, the time horizon in forecasting is generally limited. In particular, in the case of the COVID-19 epidemic, uncertainties about changes in the epidemic process, e.g. the potential emergence of new variant, or in human behavior through e.g. alterations of governmental regulations, heavily limit the viable forecast horizon and hence, horizons were generally limited to four weeks in most forecasting efforts \cite{reich_collaborative_2022}. %In fact, in subsequent analyses of COVID-19 forecasts, it was generally found that forecast performance for incidence cases usually sharply drop after only two to three weeks \todos{(cite)}.\\
We briefly also mention the practice of scenario modeling, which, in contrast to forecasting, seeks to elicit future trajectories of the quantity of interest under a pre-defined set of settings for the relevant circumstances affecting said quantity \citep{reich_collaborative_2022}. In the context of COVID-19, these settings can be related to, for instance, specific policy interventions, levels of vaccine availability or efficacy, emergence of new variants of the virus, or any combinations of these. Scenario modeling can be used to inform decision makers that seek to evaluate the plausible effects of potential strategies, e.g. specific disease control measures, under a variety of circumstances \cite{reich_collaborative_2022}.\\ 
In general, forecasts can directly be evaluated against the truth data that realized, while scenario models are harder to evaluate, as it is generally unlikely that the settings characterizing/underlying the scenario actually realized in the exact way they were defined..
\subsection{Ensemble Forecasts}
Rather than relying on a single model's output for forecasting, the practice in many fields nowadays is to instead rely on aggregation strategies that unify and leverage individual models' respective predictions into a single forecast, thereby aiming to take advantage of individual models' strengths, be it superior modeling strategy or a better information set.\\
%An ensemble aggregates models, thereby 
Basic intuition for why combining a set of forecasts rather than relying on a single forecasting model derives from the fundamental statistical fact that averaging over a set of independent measurements estimators $X_i$ that are in principle unbiased estimators of a quantity $\mu$ (E($X_i$) = $\mu$) (but with measurement error $\epsilon$) still gives an unbiased estimate of that quantity, but with reduced variance. That is, through combining forecasts that we believe to on average yield accurate predictions for the quantity of interest, we somewhat cancel out the forecast errors they individually make at a given time point in practice - this of course assumes that the forecast errors they make are not correlated. While these conditions of unbiasedness and uncorrelatedness are of course unlikely to completely hold in practice, \cite{timmermann_chapter_2006} argues that even in these cases, combining forecasts still provides benefits over relying on a single forecast as a general practice. For instance, forecasts might be individually biased at certain time points, possible reasons for which might be  model misspecification or data errors. Such a bias might however be hard to detect in real-time, making ensemble forecasts a robust mitigation strategy, as other models presumably did not suffer from the same issues and can counterbalance that single prediction. Furthermore, forecast errors might be correlated, for instance following an unforeseen intervention, but one can still expect individual biases to at least somewhat cancel out. In fact, the benefit of combination can be presumed to be the greatest in the presence of structural breaks, where any single models might have substantial drops in accuracy / models might have very variant reactions to these breaks, which can be mitigated through averaging several models \citep{timmermann_chapter_2006}. Additionally to this, another reason for increased performance through combining forecasts could be that one can expect increased performance due to varying information sets underlying the different forecasts that can be leveraged through combining the forecasts.\\
This theoretical argument is backed up by empirical results - in practice, ensemble forecasts are widespread and usually show superior performance to single models. This result holds across fields, e.g. weather forecasting, economics, and also epidemic forecasting.
%They have a long tradition in weather forecasting, where they show consistently improving performance over single models. \\
Ray2020: "Multiple studies of epidemic forecasting have shown that ensemble forecasts, which incorporate multiple model predictions into a combined forecast, consistently perform well and often outperform most if not all individual models (Viboud et al. 2018; Johansson et al. 2019; McGowan et al. 2019; Reich, Brooks, et al. 2019)." Conversely, \cite{bracher_pre-registered_2021} did not find that the ensemble outperformed single models. \cite{the_influenza_forecasting_working_group_collaborative_2019} found that a simple average ensemble outperformed most models, but some top performing models still consistently outperformed the ensemble.\\
In some cases, these are ``Hub'' ensembles, but even when it's ``only'' teams incorporating multiple in-house models (and the diversity in knowledge/strategies is thus presumably more limited), this was associated with increased performance benefits.\cite{the_influenza_forecasting_working_group_collaborative_2019} and \cite{reich_collaborative_2019}. \\
However, a question that remains is which concrete method to use for combining forecasts into an ensemble - in particular, whether to give all models equal weight in an ensemble or whether it is better to give models with a proven track record of good performance higher weight. In practice, ensemble forecasts with estimated weights often perform poorly relative to unweighted aggregation methods, a fact that came to be known as the ``forecast combination puzzle'' - see \cite{claeskens_forecast_2016} for a theoretical argument for why this might be the case.\\
Nevertheless, in recent history of infectious disease forecasting, different results have been obtained with regard to this question: \cite{sherratt_european_2022} have shown that the unweighted median outperforms individual models, which is a common result, but in the case of the US forecast hub, .... For the case of forecast for COVID-19 deaths in the US, \cite{taylor_combining_2021} make the case that weighted methods are somewhat hindered by the fact that not all models have a consistent record of forecast performance, making estimating weights based on that past performance difficult. This is also the case for the European forecast hub - nevertheless, the question arises whether we can somehow build an improved ensemble.\\
...
A question that thus remains is whether unweighted ensembles are really the way to go for the European Forecast Hub.
\subsection{US forecast hub}
Numerous studies have come out for the US forecast hub. Taylor and Taylor, Ray 2020. \cite{ray_ensemble_2020}.
\subsection{Epidemic forecasting}
There are a few characteristics that can make forecasting epidemics particularly challenging: firstly, epidemic models seek to model transmission dynamics, which are not constant and can change during the course of an epidemic, namely (but not exclusively) due to human behavior changes and/or changes in the relevant pathogen \citep{moran_epidemic_2016}. Furthermore, data sources come with their challenges: While it is clear that timeliness is of critical importance for disease surveillance, data are oftentimes not readily available in real-time, and are sometimes made available only months after initial recording \cite{jajosky_evaluation_2004} \todos{(more directly cited from Moran)}. Different data streams, all with own challenges \cite{moran_epidemic_2016}. Briefly describe truth data collection in Covid-19 epidemic. One could argue that in the recent Covid-19 epidemic, there were major improvements in the timeliness of reporting, yet data continued to see retroactive revisions, sometimes substantially \todos{(cite)}. \\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%MODEL TYPES: THEORETICAL#######################################
%\subsubsection{Model types in epidemic forecasting}
Various different models and methodologies can be used to issue forecasts, and epidemic forecasting is no exception in this regard. The most common models used in epidemiology and within the data used for this thesis can broadly be sorted into categories, which we will briefly introduce and discuss in the following. For sorting models, we consulted the metadata files that teams submitted into the Hub, which gave details about their model and fitting process among other things, such as the truth data source they used to train their model on. Furthermore, for many modeling teams, additional resources could be found, such as websites or their own publications about the model, which we also consulted.\footnote{The spreadsheet/csv file detailing the categorizations can be found in the Github repository of this thesis, in the `scraper' directory.} %any publications dealing with the relevant model - 
Furthermore, we stay closely aligned to \cite{bracher_pre-registered_2021} who used the same categories when sorting their models. We will give some examples of models that fit into these categories from the Hub data\footnote{For a full list of models from the European COVID-19 forecast hub, consult https://github.com/covid19-forecast-hub-europe/covid19-forecast-hub-europe/tree/main/data-processed}. However, note that categorization of models has been done differently beforehand - e.g. \cite{funk_assessing_nodate} denote a model as semi-mechanistic that has explicit compartmental structure.\\
Compartmental - alternatively named mechanistic - models are among the most widely used models in epidemiology to model infectious diseases. The term ``compartmental'' derives from the fact that these models divide the population into compartments according to their infection status with respect to the infectious disease at time $t$. In their most basic structure, these compartments are $S$, $I$ and $R$ \citep{brauer_epidemic_2012}: $S(t)$ denotes the number of individuals that are susceptible to the disease at time $t$, that is, they can potentially be infected with the disease. $I(t)$ denotes the number of infected individuals, which are assumed to be able to spread the disease when in contact with individuals from compartment $S$. Finally, $R(t)$ denote the number of individuals that, after being infected, have been removed from the process at time $t$. This compartment thus contains individuals that are either isolated, recovered/immunized without possibility of reinfection, or dead as a consequence of the disease. Additional characteristics of the epidemiological process of the disease, e.g. the availability of a vaccine, can be modeled via extra compartments. \todos{(this is a weird sentence)}. Commonly added extra compartments are $E$ for exposed but not yet infectious individuals, giving an SEIR model, or an additional $S$ compartment to model non-permanent immunity from the disease, resulting in an SIRS model \citep{brauer_epidemic_2012}. \\
Given some regularity assumptions and a set of parameters governing the transmission process, flow of individuals between the compartments is modeled via a set of differential equations, from which key parameters again derive - most notable among them is the reproduction number $R_t$, which denotes the average number of susceptible individuals that an individual from compartment $I$ infects at time $t$. However, the reproduction number is not an explicitly modeled parameter in these frameworks.\\
Furthermore, there exist models that are built on epidemiological information but do not have an explicit compartmental framework - these models we sort into the \textit{semi-mechanistic} category. This category most notably includes models that model cases or deaths via growth rate approaches, but also contains models that are based on other epidemiological principles, for example the renewal equation. For the former, the general strategy is to estimate a time-varying growth rate, which is then mapped to predicted infections.\footnote{the reproduction number $R_t$ and the growth rate $r_t$ are separate, but related quantities} Deaths are then usually modeled as a fraction of infections. The texttt{LANL-GrowthRate} model estimates. The \texttt{EpiNow2} model models the reproduction number via a non-stationary Gaussian Process,  while the . \\
Another strategy for forecasting in epidemiology are statistical models. %While the aforementioned methods of course also utilize statistical methods in estimating key parameters,
This category refers to models that rely solely on statistical methods (\todos{noch nicht gut genug abgegrenzt}) and are thus agnostic to the underlying transmission dynamics - put succinctly, \cite{holmdahl_wrong_2020} refer to this strategy as ``crunch[ing]'' epidemiological data from the past [...] and project[ing] cases into the future''. These model are mostly time series, for example ARIMA models, such as the \texttt{MUNI-ARIMA} model in the Hub. However, they can also be ...\\
In practice, this distinction between model types is not entirely clear. As noted by \cite{reich_collaborative_2019}, for instance, compartmental models often have statistical components - this is also the case for a lot \todos{(if not all?)} of the models in the Hub. We thus followed \cite{reich_collaborative_2019} in categorizing any model as mechanistic which had an explicit compartmental framework.\\
Before eventually turning to our analysis of model type performance in the Hub data, we first want to highlight/discuss some previous findings with regard to this question in the context of other diseases, as well as discuss some reasonings that argue for or against model types on a purely theoretical basis.\\
Overall, these model types all have their differing advantages and disadvantages. Due to the fact that they rely solely on past time series data to make predictions, statistical models' predictions are usually only suited for forecasting (recall: predicting for short time horizons under the current status quo), while mechanistic models are also suited for longer term planning via scenario modeling \citep{reich_collaborative_2022}. Furthermore, as mechanistic models explicitly attempt to model underlying transmission dynamics, they can generate insights of the process that purely statistical models cannot \cite{james_use_2021} \todos{(check if this was actually said here)}. On the flip side, this means that model accuracy is also somewhat limited by knowledge about the virus \citep{holmdahl_wrong_2020}. Furthermore, when discussing model type advantages and drawbacks for the case of influenza, \cite{reich_collaborative_2019} make the argument that mechanistic models describe a specific disease transmission process, but that real-world data gathered on the disease is substantially influenced by factors (e.g. testing behavior) which are at best only marginally related to that process - this might give a comparative advantage to statistical models working directly on official count data. Even though influenza and COVID-19 of course have structural differences, we argue that this ``gap'' between actual transmission and the recorded disease numbers (through e.g. incomplete testing) was likely also present in the case of COVID in most locations and thus might also play a role for model performance in our analysis. However, this is ``easily'' addressed by extra statistical components .. Contrary to this, \cite{bracher_evaluating_2021} argue that within the setting of emerging diseases (rather than seasonal diseases such as influenza), mechanistic models might have an advantage - this is due to the fact that the limited amount of historical data could be more of a limit for statistical models - influenza (very roughly) follows seasonal patterns, which statistical models can learn given enough data. Furthermore, there is an issue with identifiability: it is harder for statistical models to accurately predict the effect of new interventions, as they have no past data to base the estimation on. Mechanistic models have key parameters that researchers can set or influence that have a real and interpretable role in the context of the transmission system that is modeled and seeks to approximate the reality.\\ 
This imperfect data basis might benefit mechanistic models, presumably as statistical models are more likely to pick up on spurious fluctuations within the data, thereby leading to flawed predictions. Lastly, a theoretical problem of growth rate approaches could be that they - thus, overpredicting the growth rate , while underpredicting it by the same nominal \todos{(use different word)} amount would get y- we presume that this could especially be a problem at longer horizons. Thus, we can say that from a theoretical point of .\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Overall, it can be said that there is no consensus in the epidemic modeling community on which of these model types performs best \cite{moran_epidemic_2016}. However, one might presume that given their different strategies, they might perform better in certain and .  \\
Of course, this categorization of modeling strategies is not entirely clear-cut and also not  exhaustive: furthermore, there exist agent-based approaches, which model epidemiological processes by simulating behavior of each individual - \cite{zelner_accounting_2021} state that these can provide useful insight but can be very difficult to fit to data. In fact, within the subset of the Hub data used in the thesis, only two models are agent-based and only for Poland. These only predict for Poland, presumably as they require a lot of tuning. \\
Moreover, there exist various efforts in the community to establish approaches that are based on human judgment, by either aggregating forecasts from experts or by random peeps. \cite{bosse_comparing_2021-1} saw promising results from a crowd forecast, which outperformed ensemble methods for forecasting cases (but not deaths) - critically, forecasts based on human judgment are thought to be better able to anticipate changes in trends, as they are, for instance, able to incorporate knowledge about changes in policy in a way more rigid modeling strategies are not \cite{bracher_evaluating_2021}. This is especially valuable due to the fact that other models have almost universally been found to be quite bad at predicting changes in trends \todos{(cite some Reichlab paper that says this, as well as Bracher DE-PL)}. One could thus conceive an idea whereby one combines expert judgment and modeling output in a way that best takes advantage of their respective strengths: use human expert forecasts as an ``alarm bell'' for trend changes and thereby rely more on them in such times, resort to ``normal models'' when alarm bell is not sounded. This is also valuable due to the fact that crowd forecasts have been found to be overly confident \cite{bosse_comparing_2021-1}. Namely, the tradition of expert forecasts has some previous tradition in the field of Economics/Econometrics: the Survey of Professional Forecasters often serves as a powerful benchmark in the field \citep{faust_forecasting_2013}.\\
Since the availability of agent-based models and expert judgment models is very limited, we mainly focus on the models in the categories mechanistic, semi-mechanistic and statistical. \\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Scoring}
In this chapter, we introduce the concept of assessing forecast performance, via e.g. proper scoring rules and other measures. We thus intend to give an overview of the toolkit available to researchers aiming to assess whether their considered models give good and accurate predictions. We introduce these, explain their characteristics and give short examples of how to interpret them.\\ 
As stated previously, the forecasts that are underlying within this thesis are issued as predictive distributions, in the form of a set of discrete quantiles. Formally, we write that the forecast distribution for model $m$ at time $t$ $F_{m,t}$ is comprised of a set of $23$ quantiles for horizons $h$ between one to four weeks ahead: $q_{i, t, h, \tau}$, where $\tau \in \{0.01, 0.025, 0.05, 0.1, ... 0.9, 0.95, 0.975, 0.99\} $. Note that quantile levels are not equally spaced, giving the predictive distributions a higher resolution around their respective tails. These quantile levels directly induce 11 central prediction intervals with nominal coverage level $(1-\alpha)\%$, with $\alpha \in \{0.02, 0.05, 0.1, 0.2, ... 0.9\}$.\\
This is the common format of issuing predictive distributions within the COVID-19 world and consequently, methods of assessing and scoring model performance have been readily developed from the more common integer-based forecasts into the quantile world. \\
Before we turn to concrete, we briefly introduce the concept of sharpness subject to calibration, that is, the forecasting paradigm that is at the basis of the way forecasts are scored and assessed in the current times.\\
\subsection{The forecasting paradigm}
Sharpness subject to calibration.\cite{ray_ensemble_2020}.
\begin{figure}
\centering
\includegraphics[width = 0.95\textwidth]{../plots/sharp_calib.pdf}
\caption{Illustration of the concepts $calibration$ and $sharpness$, as introduced by \cite{gneiting_strictly_2007}, AGAIN OTHER PAPER}
\label{fig:sharpcalib}
\end{figure}
To better understand what an optimal forecast looks like and, in the same vein, how to best assess forecast performance, we follow the established and ubiquitous paradigm defined by \cite{gneiting_probabilistic_2007}: an optimal forecasts should maximize \textit{sharpness subject to calibration}. We will now briefly explain this concept.\\
Consider that we aim to make a probabilistic forecast $F_t$ for a quantity $y_t$ at time $t$, which follows the distribution $G_t$. The ideal forecaster would thus issue 
\begin{equation}
	F_t = G_t
\end{equation}
as their probabilistic forecast. In practice, $y_t$ will eventually be observed, while $G_t$ remains an unknown and hypothetical theoretical concept. The skill of the forecaster thus needs to be judged based on the forecast-observation pairs $(F_t, y_t)$ - \cite{gneiting_probabilistic_2007} suggest to base this assessment on the concepts calibration and sharpness, which are visualized in Figure \ref{fig:sharpcalib}. \\
In their words, calibration refers to the statistical consistency between the predictive distributions $(F_t)_{t = 1,2,..}$ and the observations $(y_t)_{t = 1,2,..}$ and is comprised of several different modes - we will mention them as they become applicable, but refer to \cite{gneiting_probabilistic_2007} for a full characterization of the concept. %Perhaps the most relevant in practice, marginal calibration refers to the limits of the two distributions being equal. 
Sharpness is a feature of the predictive distribution only and simply refers to the concentration of the predictive distribution. In short, they establish the result that the notion of an ideal forecaster is equivalent to the forecaster maximizing sharpness subject to calibration. In turn, this means that calibration is not a sufficient, but a necessary condition of the forecast. When assessing forecast performance, both characteristics thus need to be judged and accounted for.\\
\subsection{Proper scoring rules}
Proper scoring rules are ideal for this task, as they incorporate this idea perfectly.\\
Now, some tools for assessing calibration. In general, calibration should be seen as a necessary rather than a sufficient condition for forecast optimality.
\subsection{PIT histograms}
\subsection{Coverage}
This is a measure of marginal calibration, that is, the limits of the two distributions being equal.
%For defining optimal forecasting and threby laying groundwork on how best to assess forecast performance, 
Then, scoring rules:\\
\subsection{Proper scoring rules}
Suppose that $y$ is the realization of a random variable under the true data-generating distribution $G$. The forecasting problem is defined by trying to issue a predictive probability distribution $F$ for the future realisation of this random variable. Further, denote $s(F,G)$ for the expectation of $\text{E}[s(F,y)]$. We then say that scoring rule $s$ is \textit{proper}, if 
\[s(G,G) \leq s(F,G).\]
Put into words, this means that the scoring rule is minimized if the true data-generating distribution is issued as the forecast distribution. Likewise, the scoring rule $s$ is \textit{strictly proper}, if 
\[s(G,G) < s(F,G).\] 
A (strictly) proper scoring rule thus incentivizes the forecaster to issue his or her true belief for the predictive probability distribution.\\
This notion of the propriety of scoring rules originated with \todos{Winkler and Murphy (1968)} and its importance in the forecasting world (hmpf) cannot be overstated - if a scoring rule for distributional forecasts is not proper, it could, for instance, incentivize a forecaster to report a more confident estimate than he or she actually believes in \todos{Thorarinsdottir 2013}. \\
Mention that aggregating across locations/... preserves propriety \citep{bracher_evaluating_2021}.\\
We also want to note here that the specific scoring rule used in an application is not a meaningless choice: as will be demonstrated in later sections, different scoring rules in practice usually induce different rankings of forecasts. In practice, the guidance is thus usually to consider several scoring rules - sometimes, a stakeholders' or decision makers might also exhibit a certain preference for forecast performance, which can guide the choice.\\
Recall the concept of sharpness subject to calibration: scoring rules assess this.
\subsection{Judging forecast performance}
We focus on evaluating model performance via two central principles, that is, accuracy (coverage/calibration) and overall predictive performance (weighted interval score). \citep{sherratt_predictive_2022}.
\subsubsection{Assessing calibration}
In general, we call a forecast that is overdispersed underconfident or conservative.
\subsubsection{PIT}
One way to assess a forecast's probabilistic calibration, that is, the congruence of the proportion of observations falling below a given threshold as observed and as they are predicted by the forecast (that is, the predictive distribution's quantiles), is through the probability integral transform (PIT) histogram \citep{dawid_present_1984}.  \\
The PIT is simply defined as the predictive cumulative distribution function's value at the observed value, that is, $p_t = F_t(y_t)$. If the forecast has good probabilistic calibration, we can expect a histogram of these transformed values to be approximately uniform. A \\
However, these can be seen as a necessary, rather than as a sufficient condition for the validity of a forecast model \cite{gneiting_probabilistic_2007}. For example, a weather forecaster that always predicts with a location's yearly stationary temperature distribution would, if also judged at yearly resolution, receive a uniformly shaped (assuming no structural shifts in the temperature's distribution\footnote{Do I hear you say climate change?}), but would be of no real use.
\subsubsection{Coverage}
Coverage assesses forecast calibration by measuring the proportion of observations that fall into the predictive distribution's respective ranges over time and comparing this to the ideally expected coverage, the $nominal$ coverage. Coverage is thus a good assessment of probabilistic calibration of forecasts.
\begin{enumerate}
\item (Empirical) Interval coverage measures the proportion of observations that fall into a central prediction interval of the distribution. For instance, for the $50\%$ prediction interval, all observations that fell within the predictive distribution's $25\%$ and $75\%$ quantiles at the respective points in time are counted and divided by the total number of observations - the goal is to be as close as possible to the nominal coverage rate of $50\%$. By assessing this for several central prediction intervals, we can additionally assess marginal calibration, that is, the congruence of the limit\footnote{We are of course only approximating by our sad finite sample ways.} distribution of the predictions with the true limit distribution as exhibited by the observed data \cite{gneiting_probabilistic_2007}. Within the literature around forecasting COVID-19, it is common to assess empirical interval coverage at the $50\%$ and $90\%$ or $95\%$ levels, to assess both the central tendency as well as the tail behavior of the predictive distributions.
\item Quantile coverage measures the proportion of observations that fall below a given quantile of the predictive distribution. It thus conveys more information than interval coverage, which does not distinguish between the upper and lower quantile of the central prediction interval. For instance, a predictive distribution could exhibit good performance at the lower tail end of the distribution, but not at the upper end, which we could thus assess. Similar to assessing PIT histograms \cite{bosse_evaluating_2022}.
\item Coverage deviation summarizes the above by averaging over the observed deviations between the empirical and nominal coverage for a set of central prediction interval. We thus obtain a parsimonious way to assess coverage across several intervals, but run the danger of losing (or even masking?) information (i.e. which interval is better).
\end{enumerate}
%%%%%%%%%%%%Then, show coverage plot of three examplary models%%%%%%%%%%%%%%%%%%
Also here, as one could just predict the historical average. At the same time, one needs a sufficiently large sample to assess coverage. Thus, a model can ``hide'' behind good coverage in the aggregate. Assessing calibration in general is thus more a necessary rather than a sufficient condition for good forecast performance. However, it is still a very useful diagnostic tool that can help forecasters assess whether they are generally over- or under-confident.\\ 
%Prediction interval coverage measures the proportion of values that fell into a predictive interval of a given level and thus reflects how well a model was able to characterize uncertainty over time.\citep{cramer_evaluation_2022} \todos{(from SI, cite something better. Scoringutils paper is also a good reference)}. It measures probabilistic calibration \citep{bosse_evaluating_2022}.
In related literature, it's common to report coverage for the central $50\%$ and $90\%$ prediction intervals \todos{(cite)}.
\subsubsection{Bias}
To assess whether a forecasting model is biased, that is, systematically over- or underpredicts the target of interest, one can utilize the bias metric as proposed by \cite{funk_assessing_nodate}. For integer-valued forecasts, they suggest the following metric:
\begin{equation*}
B(F_t, y_t) = 1 - (F_t(y_t) + F_t(y_t + 1)).
\end{equation*}
In terms of bias, the ideal forecast would have exactly half its probability mass above and below the true value $y_t$, respectively. If this is exactly the case, the metric assigns a value of zero - otherwise, if the probability mass is unequally distributed, the forecast receives a penalty. If more probability mass lies below  the true value than above it, bias is negative - the extreme case thus occurs when the entire probability mass lies below true value, where we get $B_t = -1$. The opposite applies in the case where more probability mass lies above the true value. Thus, bias measures a forecast's general tendency to relatively over- or underpredict the target \cite{bosse_evaluating_2022}.\\
\cite{bosse_evaluating_2022} extend the bias metric for quantile-based forecasts as follows. If the true value $y_t$ is below the predicted median forecast $q_{t,0.5}$, the bias is 
\begin{equation*}
B(F_t, y_t) = 1 - 2 \cdot \text{max}\{\tau | q_{t,\tau} \leq y_t \}.
\end{equation*}
Similarly, if the true value $y_t$ is above the median forecast $q_{t,0.5}$, it is
\begin{equation*}
B(F_t, y_t) = 1 - 2 \cdot \text{min}\{\tau | q_{t,\tau} \geq y_t \}.
\end{equation*}
This can be interpreted as twice the difference between the quantile level that would ideally be closest to the observed data $y_t$ ($\tau = 0.5$, since the median would be the ideal prediction) and the quantile level that is actually most in line with it. If the observed value directly coincides with the median prediction $q_{t, 0.5}$, bias is zero, otherwise, the forecast receives a penalty. Concretely, if we interpret the quantiles as the endpoints of (central) prediction intervals, this is the outermost quantile of the predictive distribution such that the interval still contains the observed value $y_t$. As above, if the entire predictive mass is above or below the observed value, bias should take on the values 1 and -1, respectively. This is achieved by simply setting $q_{t,0} = 0$ and $q_{t,1} = \infty$.\\
Thus, in short, the further the central mass of the predictive distribution is from the observed value, the larger the bias.\\
An important feature of the bias metric is that it is bound to the interval $[-1,1]$ and thereby scores forecasts on a relative rather than an absolute scale - this means that we can directly compare different targets, even if their value ranges differ substantially.
In the next subsection, we introduce the weighted interval score, which also includes penalties for over- and underprediction and can thus be used to assess a forecast's bias. However, as \citep{bosse_evaluating_2022} remark, it does so on an absolute scale - the potential issue with this is that there are no ``ideal'' absolute values for over- and underprediction and these values can thus not be directly translated into an assessment of systematic bias. To give an example, a forecast might overshoot a particular target during high incidence times, leading to a high absolute penalty in terms of overprediction, which will dominate its overall scores - this however might mask a potential tendency of that some forecast to normally underpredic, which will be reflected in the bias metric.\\
``It largely depends on the application whether one is more interested in the tendency to be biased or in the absolute value of over- and underpredictions'' \citep{bosse_evaluating_2022}.
\subsubsection{Weighted Interval Score} \label{ssub:weighted_interval_score}
Here, we introduce the \ac{wis}, which is the main scoring rule used within this thesis \cite{bracher_evaluating_2021}. It is designed for use on probabilistic forecasts \cite{european_covid-19_forecast_hub_european_2021} $F$ that are issued as a set of discrete central prediction intervals, each with nominal coverage level $\alpha$ - or, put differently, as a set of symmetric predictive quantiles $q$ which directly translate to central prediction intervals. \\
Each central prediction interval can be scored via the interval score \citep{gneiting_strictly_2007}
\begin{equation}
IS_{\alpha}(F, y) = (u-l) + \frac{2}{\alpha}(l - y)\mathbb{1}(y < l) + \frac{2}{\alpha}(y - u)\mathbb{1}(y > u),
\end{equation}
where $\mathbb{1}$ is the indicator function, returning 1 if the condition inside the parentheses is fulfilled and 0 otherwise. The three summands each have an intuitive interpretation. The second and third summands express under- and over-prediction, respectively. They assign a penalty if the true observed quantity $y$ falls below (above) the lower (upper) endpoint $l$ ($u$) of the prediction interval. The first $(u-l)$ expresses the width of the central prediction interval and thus the sharpness of the predictive distribution $F$ - if this term didn't exist, it would make sense to simply issue very large prediction intervals that are highly likely to contain the true observation $y$. These penalties are furthermore scaled by the nominal coverage level: a smaller $\alpha$, which corresponds to a higher nominal coverage rate, induces a higher penalty if $y$ does fall outside one of the endpoints. \\
\cite{bracher_evaluating_2021} extend this score for use on a predictive distribution $F$ that consists of a set of such intervals, each with unique coverage level $\alpha$. The set of interval scores is gathered and aggregated into the weighted interval score 
\begin{equation}
WIS_{\alpha_{0:K}}(F,y) = \frac{1}{K + 1/2}\left(w_{0}|y-m| + \sum_{k=1}^{K}\left(w_k IS_{\alpha_{k}}(F, y)\right)\right),
\end{equation}
where usually the quantile weights are set to $w_k = \frac{\alpha_{k}}{2}$, and the median weight to $w_{0} = \frac{1}{2}$.\\
It can be shown that the \ac{wis} is an approximation of the \ac{crps}, a well-known scoring function that measures the distance between the predictive and true distribution 
\begin{equation}
CRPS(F, x) = \int_{-\infty}^{\infty} \left(F(y) - \mathbb{1}(y \geq x) \right)^2dy.
\end{equation}
In contrast to the \ac{crps}, the WIS gives slightly larger weight to intervals with large nominal coverage \citep{bracher_evaluating_2021}.
All in all, the \ac{wis} is a parsimonious way to score forecasts that come in the shape of a set of discrete intervals \citep{sherratt_predictive_2022}. Its decomposition allows to understand it directly as summarizing the trade-off between coverage and precision. \\
An important characteristic of the WIS is that it is not standardized and thus scales with the data. This can be easily seen in equation ?, as the absolute differences of the observed value and the predicted quantile directly enter into the score. Thus, scores will naturally increase if the target to be predicted increases. This makes forecast comparisons a bit difficult, which leads us to the next point.
\subsection{Comparisons of forecasts}
An issue that often arises when aiming to compare different forecasting models is a potentially non-overlapping base of targets the models issued predictions for. As most scoring rules are not normalized and scale with the data, %For instance, if models were compared via average WIS, 
one model might look better than another in terms of simple average scores if it only predicted in periods that saw low incidence or that were otherwise comparatively "easy" to forecast. This would thus disincentivize forecasters to predict in periods that they perceive to be more challenging - this is especially undesirable because these periods (e.g. exponential growth, high level of infections) are often of special interest to decision makers \todos{(cite something)}.\\
One can address this by computing a relative score that is based on pairwise comparisons of models, as developed in \cite{cramer_evaluation_2022}. For a pair of models denoted $l$ and $m$, first a measure of relative skill is computed
\[
\theta_{l,m} = \frac{\bar{s}_{l}}{\bar{s}_{m}},
\]
where $\bar{s}_{l}$ and $\bar{s}_{m}$ denote the average scores the models achieved on the targets both models predicted on. In principle, one can base these comparison on any scoring rule that gives either exclusively positive or exclusively negative values, but we follow the relevant literature and thus the WIS.\\
For each model $l$, one then computes the 
\[
\theta_{l\cdot} = \left(\prod_{m = 1}^{M}\theta_{l,m} \right)^{\frac{1}{M}},
\]to obtain a relative score of model $l$ with respect to all other available models. The choice of the geometric mean as an aggregation function over models is convention in the literature, but \cite{ray_comparing_2022} show in their application that the choice of aggregation function is indeed not critical, and that one could thus also use an arithmetic mean. \\
This relative score can be interpreted as a performance measure of model $l$ with respect to a model with ``average'' performance. If interest lies in a direct pairwise comparison with a specific model $m$, one can instead consider the ratio of these relative scores
\[
\phi_{l,m} = \frac{\theta_{l\cdot}}{\theta_{m\cdot}}.
\]
Calculating this ratio for all model pairs that are of interest results in a ``pairwise tournament'' for all models in the set - this approach is implemented in the \texttt{scoringutils} package \citep{bosse_epiforecastsscoringutils_2022}. For negatively oriented scoring rules, the ratio will be smaller than 1 if model $l$ outperformed model $m$ on their set of shared targets and larger than 1 if it did not. Note that this mode of pairwise comparison still requires the assumption that it is equally hard to perform relatively well to other models at all forecast dates and locations \citep{cramer_evaluation_2022}.\\
If one is interested in concisely summarizing the skill of single models rather than performing comparisons between all pairs of models, one can choose a baseline model's $B$ relative score $\theta_{B\cdot}$ as the denominator. When using the WIS as the scoring rule, this results in the measure that is commonly referred to as "relative WIS" \citep{cramer_evaluation_2022}. Analogously to above, a ratio below 1 corresponds to a model overall outperforming the baseline model, while a score above 1 means that the model did not succeed in clearing baseline performance.\\
In section \ref{sub:model_types_analysis}, we use these comparisons to compare groups of models, more specifically certain model types, against each other (better: compare relative skill?). As stated previously, averaging scores preserves propriety, making this comparison possible.\\
This method of comparing component forecasts was largely developed to address missingness in component forecasts and can thus be used to compare a larger number of models with differing amount of availability. When interest instead lies in directly comparing the performance of two specific models against each other, especially when one model is a more established ``benchmark model'' that the other model wants to win against, it can be more direct and natural to directly compare the average scores the two models obtained over all considered forecast dates and locations. This is also the approach taken by e.g. \cite{bosse_comparing_2021-1}.\\
%Lastly, we mention that missingness is only a problem that affects the component forecasters. In section XX ("Ensemble Experiments"), our ``alternative ensembles'' have perfect availability throughout the whole study period. We therefore feel it is more natural to directly scale the average aggregate scores the competing method obtained with those of the unweighted median that includes all component forecasters. As we are fundamentally interested in establishing an alternative to the already established benchmark rather than over individual component forecasts.\\
%``To enhance interpretability of scores we mainly report WIS relative to the Hub ensemble in the main text, i.e. we divided
%the average scores for a given model by the average score achieved by the Hub ensemble on the same set of
%forecasts (with values >1 implying worse and values <1 implying better performance than the Hub ensemble).''\cite{bosse_comparing_2021-1}.\\
Importantly, while this style of forecast comparison addresses the problem of non-overlapping models, it is still based on average scores and is thus most heavily influenced by targets with large nominal scores. Wherever possible, we thus 
\subsection{Model distance}
We so far discussed scoring rules, which, as stated, are used to assess the pairs $(F_t, y_t)$. In contrast to this, divergence functions $d$ are used to assess distance between a pair of distribution functions ($F_t$, $F'_t$) \citep{thorarinsdottir_using_2013}. This can be used to score the predictive distribution $F_t$ against an estimate of the true data generating distribution $\hat{G}_t$, but also to measure the distance between two competing predictive distributions $F_t$ and $H_t$. As it is fundamentally a measure of distance, the divergence function needs to fulfill $d(F,F)$ = 0 and $d(F,H)\geq 0$ for all $F$ and $H$. Furthermore, as they can be used to score a predictive distribution, it also needs to fulfill the notion of propriety as defined previously. Analogously to before, this means that. \\
The distance function we will use within the context of this thesis is the 2-order Cramer distance or integrated quadratic distance \cite{thorarinsdottir_using_2013}. It is defined as the integral
\begin{equation}
CD(F, H) = \int_{-\infty}^{\infty}\left(F(t) - H(t) \right)^2dt
\end{equation}
Incidentally, this is the divergence function associated with the CRPS as defined previously - this means that if a point forecast is issued instead, it reduces to the CRPS.\\
\cite{wang_covidhubutils_2022} developed an approximation of the Cramer distance for (potentially unequally-spaced) quantile-based forecasts as follows:
\begin{equation}
CD(F,H) \approx \sum_{i = 1}^{2K-1} (a_i^F - a_i^H)^2 (q_{i+1}^{P} - q_{i}^{P}),
\end{equation} 
where $q^P$ contains the $2K$ pooled and ordered predictive quantiles\footnote{With a slight abuse of notation as the subscript $i$ in this case indexes the position in the vector $q^P$, whereas usually the subscript refers to the quantile level $\tau$. However, since $\tau \in (0,1)$ and $i \in \{1,2,...,2K\}$, we believe the distinction is clear enough.} from $F$ and $H$ and $a_i^F$ is a vector that, for each quantile $q_i$ in the pooled quantile vector, contains the minimum quantile level $\tau$ for which the associated quantile $q_\tau^F$ is larger than it. We write this condition as
\begin{equation*}
a_i^F = \text{min}\{\tau | q_\tau^F \geq q_i^P\},
\end{equation*}
with an analogous definition for $H$.
Some characteristics: on the scale of the data $y_t$.\\
Explain some characteristics of the Cramer distance, i.e. that it gives small distance to distributions that are wide. Investigate this a bit.
\section{Data}
The data used in this thesis stem from the European COVID-19 Forecast Hub (thereafter referred to as the ``Hub'' or ``European Hub'', for the sake of brevity), which was instigated by the \ac{ecdc} in 2021 to collate forecasts for COVID-19 incidence cases and deaths from independent modeling teams across Europe \citep{european_covid-19_forecast_hub_european_2021}.\footnote{info is taken from the Wiki at https://github.com/covid19-forecast-hub-europe} It was modeled after a similar previous effort in the United States, the United States COVID-19 Forecast Hub (thereafter referred to as the ``US Hub'') \citep{cramer_united_2021}. Furthermore, the preceding German-Polish Forecast Hub was largely synchronized with the European Hub \citep{bracher_german_2020}. The Hub's primary goal is stated to "provide reliable information about the near-term epidemiology of the COVID-19 pandemic to the research and policy communities and the general public" \citep{sherratt_predictive_2022}.\\ 
In general, a modeling hub is a coordinated effort, in which one or more common prediction targets, as well as a common format for prediction, are agreed upon and centrally implemented \citep{reich_collaborative_2022}. This serves the purpose of facilitating model evaluation and development by making model predictions comparable, as well as making predictions suitable for aggregation, that is, for generating ensemble predictions. A central advantage of the ``Hub'' methodology is thus the potential to synthesize results from different modeling approaches and teams - for instance, \cite{metcalf_opportunities_2017} remark that, following major disease outbreaks, there is often an explosion of modeling studies, but the usefulness of the thereby generated research is limited as quality of data and methods used vary, and there is often no follow-up for synthetication of results. %, somewhat passing up the opportunity to bring the field forward. 
The Hub format, in contrast to this, standardizes data quality\footnote{To a certain degree: Teams are free to use whatever data sources they wish, but are told that forecasts will be evaluated against the publicly available JHU data and are thus recommended to base their methods on the same data source. Furthermore, the hub recommends data sources for e.g. vaccination and testing data.} and facilitates evaluation and thus synthetication of results through a set of shared targets. The format has some precedence both in other fields, for instance climatology or ecology \citep{warszawski_inter-sectoral_2014}, as well as in epidemiology itself - some notable examples include forecasting influenza in the United States \citep{reich_collaborative_2019} as well as dengue fever in Puerto Rico and Peru \citep{johansson_open_2019}. \\
\begin{figure}
\includegraphics[width = \textwidth]{plot_placeholder/visualize_data.png}
\caption{\todos{Replace with real plot.} Plot showing an example trajectory from the Czech Republic. Following the last available truth data point (on \todos{date}, modeling teams submit forecasts for one to four weeks into the future, respectively as a predictive distribution via a set of discrete quantiles. Pictured are the trajectories of the respective median predictions, with 90\% prediction intervals.}
\label{fig:czech_predictions}
\end{figure}
For these seasonal diseases, prediction targets were total number of cases in a season or the height of the peak, while in the case of the European Hub, the common prediction target are weekly incidence COVID-19 case and death counts in 32 European countries, later also hospitalization rates. Forecasts are issued in a probabilistic manner, more specifically as a set of 23 quantiles of the predictive distribution, at non-equally-spaced levels between 0.01 and 0.99. Consider Figure \ref{fig:czech_predictions} for a demonstration of the prediction format, in this case for case numbers in the Czech Republic, with the forecast date the \todos{date}. We can see that, ...\\ 
There were no restrictions for participation in the Hub, (meaning that in theory anyone could participate) and participating teams or individuals were free to only submit forecasts for any subset of combinations of locations and target types. Forecast dates were standardized and based on the \ac{ew} format as defined by the US \ac{cdc}, where each week starts on a Sunday and ends on the following Saturday. Forecasts were submitted by Monday and thus scored against the truth data as realized on the Saturday of the same week for 1-week ahead forecasts, and accordingly by the following Saturdays for larger horizons.\\ 
The truth data source that models are evaluated against stem from the \ac{jhu}, which collate and make available daily cumulative counts of cases and deaths. Since the Hub asks for weekly forecasts of incidence, the truth data is obtained by taking weekly differences of the \ac{jhu} national data. This can in theory be problematic, as data are subject to revision and negative values for incidence counts are thus possible.\\
To be included in the official Hub's ensemble, models had to provide a full set of 23 quantiles for all four forecast horizons, as well as pass a sequence of automated checks for their predictions, that is, that their quantile predictions were non-negative, integer-valued and did not cross \citep{sherratt_european_2022}.\\
\begin{figure}
\includegraphics[width = \textwidth]{../plots/trajectories.pdf}
\caption{Plot showing the trajectories of the different time series. Note that the time series for Cases were normalized by the respective location's population for readability. Time series for deaths are direct incidence counts.}
\label{fig:trajectories}
\end{figure}
\begin{figure}
\includegraphics[width = \textwidth]{../plots/availability_indivi.pdf}
\caption{Plot showing the overall availability of each model submitted to the European COVID-19 Forecast Hub across the five locations considered in this study. It can be seen that there is large variability in the availability of each model. Some models are available during most of the study period, while others only submit forecasts for a small section of the considered period. It can also be seen that there are some models that submit predictions for several or most locations, while others only predict for one specific location.}
\end{figure}
The Hub also includes a ``naive'' baseline model, which is the same as the one that is used in the US Covid-19 Forecast Hub \cite{cramer_evaluation_2022}. For each forecast date, its forecast for median incidence is equal to the last value for incidence Cases/Deaths that was observed in the most recent week. For uncertainty around the median, the other predictive quantiles are taken from Monte Carlo approximations of the empirical distribution function that is induced by the first differences observed in the respective time series \citep{cramer_evaluation_2022}. In essence, this model can thus be seen as a martingale/random walk model with non-stationary variance. On the one hand, including a baseline model serves the purpose of providing a sort of ``minimum'' performance that models should be able to clear - reporting that a model performs better than the baseline thus gives validity to the performance of that model. On the other hand, given the fact that most prominent scoring rules scale with the target and thus never obtain a sort of ``optimum'', it's a pragmatic strategy to compare performance across dimensions that are usually not easily comparable, for instance across different periods or locations. Furthermore, adding a relative model that serves as a sort of measure of forecasting difficulty rather than relying on absolute performance also serves as a sort of dis-discouragement of inviting teams to forecast during periods that are perceived as more ``difficult'' than others (and which additionally also receive nominally very high scores). However, we want to note here that while the introduction of a baseline aims to fulfill the purpose of standardizing model comparison, it is not without alternative and can skew the assessment somewhat if it systematically performs better in some dates than others: as we will see, there are some periods which the baseline fails at. As \cite{bracher_evaluating_2021} ``showed''(?), the choice of baseline can impact model assessment and is not entirely trivial.\\
For both the European and US Hub, the central communication tools lies with the Hub ensemble, which was generally found to perform best. In both cases, it was found that the median  \\
Talk about how the ``ensemble is best'' paradigm has also held here, with citations to both Eu and Us FCH.\\
Talk about issues of model availability.\\
Talk about differences in forecasting deaths vs. cases.\\
\subsection{Hub Data} \label{sub:hub_data}
\begin{figure}
\includegraphics[width = \textwidth]{../plots/availability_all.pdf}
\caption{Plot showing the overall availability models submitted to the European COVID-19 Forecast Hub across the five locations considered in this study.}
\end{figure}
For the subset used in this study, we decided ahead of our study to only use a subset of the available countries in the set. This is simply due to the fact that most of the analyses we aimed to perform required a sufficient model base as support. Furthermore, we decided to only use data up until the end of January of 2022. This is due to the fact that model base dropped around Christmas,. Moreover, beginning with the new year 2022, Omicron\textsuperscript{TM} entered the chat and in some countries, testing criteria changed. We thought that this could make the time periods less comparable.\\
Talk about availability. We can see that in the aggregate, after an initial period of uptake and recruiting, the modeling base is largest during the summer months and into the fall of 2021, after which it drops.
In general, the varying model base can be a bit of an issue. One can deal with these issues by e.g. only taking a full forecast set. However, we deliberately wanted to deal with the data as-is and develop and conceive strategies that work with and around this artefact of the data, as we felt that this was the most realistic and helpful for the situation at hand. \\
We restricted ourselves to models that submitted the full set of 23 quantiles for each of the four horizons.\\
For the US forecast hub, some things have worked well, most notably best performers or weighting. The question is whether we can also come to similar conclusions here. Especially for Death forecasts, best performers seemed to work well in the US forecast hub (see: https://covid19forecasthub.org/doc/ensemble/).\\
Some differences to U.S. Hub: most notably, the model base. We thus always stratify by location, which they don't. Baseline model is the same. Furthermore, they require 7 predictive quantiles for cases, which however does not seem like a relevant difference.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introspective stuff}
\subsection{A short note on the nature of this study}
This section, as well as the next, will largely be devoid of rigorous statistical testing. Most of the works cited in this thesis that perform evaluations of similarly are. There exist methods to test the significance of differences in forecast scores/skill between models, the most common being the Diebold-Mariano test, which however has no extension ``to account for dependencies between multiple forecasts made for different time series or locations''(\cite{bracher_evaluating_2021}, \cite{diebold_comparing_1995}). One could also conceive of a regression analysis, e.g. with random effects, but this also has the issue that scores are correlated and scores have large outliers which can heavily influence and thus weaken the trust in reported p-values. Log-transforming the scores to bring them more in line with a normal distribution and reduce the effect is also not a viable alternative, as the log-transformation does not preserve the propriety of the scores, which is a central and unremovable feature, as it upholds the ordering of the models with respect to their closeness to the ``true'' distribution \\ 
We still think there is tremendous value in the analysis, but we are aware of and want to highlight the possible limitations. We argue that some of the knowledge distilled in this thesis should be checked against another year's results, to increase trust in the results. Furthermore, \cite{bosse_epiforecastsscoringutils_2022} don't recommend statistical testing ``in applied settings''.\\ 
however, we feel that this approach is to a certain degree more honest than desperately., the results of which could also not necessarily be trusted due to the aforementioned issues...\\
We try to counteract this by carefully not masking trends in the aggregate, as well as also reporting results that did not show any striking conclusions. Most of our analysis leads were furthermore defined from the start, and we refrained from \todos{(take this out, as it's not entirely true?)}.\\
The nature of the European Hub data is such that...\\
Furthermore, our methods are to a certain degree entirely descriptive. While for the ensemble experiments we try to imitate the real-time situation as much as possible, we are aware of the fact retrospective knowledge can creep in.\\
\subsection{Model Types} \label{sub:model_types_analysis}
It is perhaps overeager to assume that one model type could systematically outperform another over the entire study period which, after all, comprises different countries with varying periods of infection dynamics.\\
First and foremost, we had to categorize the models in the data in a meaningful manner. We decided on a categorization. To this end, we . Teams that mentioned an explicit compartmental structure of SIR or related (for instance SEIR, SECIR) type we categorized as "mechanistic".\\
As stated before, the question of which modeling strategy or type is the best choice for short-term infectious disease forecasting remains an ongoing topic of research \cite{funk_short-term_nodate}. For the case of influenza, \cite{reich_collaborative_2019} found that statistical models exhibited similar or better performance than compartmental models in short-term forecasts of ``influenza numbers'' (except for 1-wk ahead) and \cite{the_influenza_forecasting_working_group_collaborative_2019} similarly found that statistical models outperformed mechanistic models, thereby suggesting that mechanistic assumptions (and thus explicitly modeling the epidemiological process) need not necessarily improve short-term predictive performance \cite{funk_short-term_nodate}.\footnote{generally, the performance differences were greater for larger horizons, include this?}. In the case of the latter study, statistical models - interestingly - performed best in relation to mechanistic ones in a period that saw a highly atypical pattern, `` illustrating that the statistical approaches may be more resilient to predictable patterns in ILI''.\\
However, as discussed previously, results from one disease don't necessarily transfer to another, especially when they show fundamentally differing characteristics, that is, seasonal vs. emerging. In the case of COVID-19, \citep{bracher_evaluating_2021} state that they did not find any ``striking patterns'' between model types in their analysis, but also acknowledge that this might be due to the relatively short study period they considered (12 weeks). A question that thus naturally arises is, whether given a longer study period, patterns can be found.\\
\begin{figure}
\centering
\includegraphics[width = \textwidth]{../plots/pw_comp_model_type_with_other_wide.pdf}
\caption{Pairwise comparison of model types. Plot was produced with package \texttt{scoringutils}}
\label{fig:pw_comp_modeltypes}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%General intro to evaluation method/data base%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Typically, the recommendation is to only compare models which are available for at least $50\% $ of the time period under study, to avoid comparison of forecasts that don't have any overlap \citep{bosse_epiforecastsscoringutils_2022}. Since we are however comparing forecasts at the level of model types rather than single models and there does not exist a pair of model types without overlap in the relevant time period, this advice does not necessarily apply here. One could still make the argument that one should not exclude models that only contribute for a very small portion of the study period, so as not to have models that more likely produce outlying forecasts\footnote{One could think that models that only have e.g. $10\%$ availability either dropped out due to not performing well and/or teams might have not been invested enough in the project to update and keep tuning their model, both of which might give low-quality predictions that as a result might not necessarily be representative of the respective model type.} influence the results too much - however, we found that the results were not at all sensitive to the exact choice of availability threshold chosen for the models, so we decided not to exclude any models here.\footnote{results from exclusions can be found in appendix} \\ 
Nevertheless, there remains the issue of the more obscure model types: as previously stated, the two agent-based models in the set only submitted forecasts for Poland, while the expert-judgment based models dropped out of the Hub during the late summer period of 2021. We report their results the highest level of aggregation (averaged results across all locations and forecast dates), but will mostly leave them out in the subsequent analyses, for conciseness and as we think that dwelling on their performance too much might give non-generalizable results. Note that the value of the mean score ratio between two model types in the pairwise comparisons is independent of other models in the set, so the mean score ratios of the other pairs are unaffected by this choice \todos{WRONGGGGGG}.\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Discuss pairwise comparisons%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The results of the pairwise comparison, where for each model type, weighted interval scores are stratified by horizon and target type, but averaged across the entire study period and all locations, can be found in Figure \ref{fig:pw_comp_modeltypes}. These plots show that the only modeling strategy that tends to show consistent increased performance improvement over other model types are agent-based models. It seems that, compared to other model types, they perform particularly well for forecasting case numbers, even more so at longer horizons. As previously mentioned, within our data set, these models only issue forecasts for Poland (for both Cases and Deaths), so the comparisons are purely based on the data from that location. Thus, it is unclear whether their superior performance would transfer to other locations or settings. In particular, most other models (and thus model types) in the data set make forecasts for multiple locations, and one could argue that designing and tuning a model for a specific location lacks some of the potential complications that arise when aiming to establish a model with more universal application. We thus argue that comparability is very limited.\\
Apart from this, we see some substantial differences in skill between expert judgment forecasts and mechanistic models for case forecasts. Upon investigation, \todos{(investigate where this comes from)} \\
We argue that perhaps the most interesting observation can be seen at horizons three and four: we see that semi-mechanistic models are outperformed by both mechanistic and statistical models for longer horizons. This could be due to the case that models based on growth rate are more vulnerable to overprediction at longer horizons, as (slight) overprediction of the growth rate, through the multiplicative nature of the underlying epidemiological process, can lead to larger overpredictions of the target. \\
  %are , so they don't necessarily receive the same level of care and effort. As previosuly sta \\
\begin{figure}
\centering
\includegraphics[width = \textwidth]{../plots/pw_comp_model_types_across_periods_wide.pdf}
\caption{Pairwise comparison of chosen model types across time periods. The code to produce this plot was adapted from package \texttt{scoringutils}}
\label{fig:pw_comp_modeltypes_byperiod}
\end{figure}
Of course, averaging across the entire study period and all locations has the issue of potentially masking some trends or effects, or worse, presenting trends in the aggregate that perhaps only spuriously result from diverging results at a lower resolution. Furthermore, as previously stated, average WIS scores are generally vulnerable to domination by periods and locations with high nominal incidence levels, thus making it possible that these results only stem from e.g. locations with larger populations.\\ 
First of all, we thus want to evaluate whether the results presented above describe a consistent trend across the entire time period, or whether these only stem from certain phases of the trajectories. We thereby follow \cite{taylor_combining_2021} in dividing our study period into 10-week periods\footnote{since we have 47 weeks in our studied sample, we actually divide into 2 10-week and 3 9-week periods}, to investigate whether these trends hold. The results can be found in Figure \ref{fig:pw_comp_modeltypes_byperiod} For conciseness, we refrain from showing all pairwise comparisons and only report performance against the baseline.\footnote{However, as can easily be seen from equation XX, if a model type has a lower mean score ratio with respect to the baseline model than another model type, it will also outperform that model type in terms of the mean score ratio.}
First of all, we can see that performance is heavily correlated across model types, that is, we observe more variations between the periods overall than between the model types. Hence, we can say that there are some periods which are harder to predict in comparison to the baseline, while during other periods this is easier. %Recall that models here are scored against a baseline model which predicts median levels to be the same in the future: this model is harder to beat in period 3, where Cases .
%%%%%%% Across horizons and locations
- we see that bad semi scores mostly seem to derive from period 5. while e.g. mechanistic models also underperform in some periods, these periods do not influence the aggregate too much.
- scores are correlated\\
- Statistical models better in period 5, for both deaths and cases. This is in line with Bracherboy \todos{(actually, not really)}.\\
Recall what we previously stated about some periods being easier to forecast in comparison to the baseline: this is especially obvious in period 2, where death numbers are mostly marked by decline and low levels overall (see Figure \ref{fig:trajectories}), we observe that all model types markedly outperform the baseline model at virtually all horizons and locations. Investigating the decomposition of baseline WIS during this period, we found that the baseline model's dispersion alone sat between 0.96 (at horizon 1) and 1.46 (at horizon 2) of the \textit{overall} average WIS score of all other model types, suggesting that the baseline model issued severely underconfident forecasts during this time. \footnote{Does this mean that the baseline model was badly calibrated? Also, find out if this was especially the case during the beginning of a period. Argument could be: baseline not so well adapted to falling numbers, where other models manage to be more confident.} Since the baseline model's uncertainty bands are based on past differences, this suggests that these don't update quick enough, marking the point that the choice of baseline is not entirely trivial. \\
\todos{(consider again the next section, might be wrong. How does averaging work again? ... See scoringutils paper.)}
To see that scores are relatively heavily dominated by periods and locations with high nominal incidence, consider the results from case forecasts in period 3 in Figures \ref{fig:pw_comp_modeltypes_byperiod} and \ref{fig:pw_comp_modeltypes_byloc}: all model types substantially underperform with respect to the baseline in the aggregate, but they actually outperform the baseline for Poland and the Czech Republic. Since these countries however have very low incidence numbers compared with other countries in the evaluation set during this period (see Figure \ref{fig:trajectories}), they don't influence the average score very much. Whether or not this is a desirable feature of the evaluation method is an ongoing debate and can depend on the forecasters' as well as decision makers' preferences. For instance, \cite{bracher_evaluating_2021} argue that scoring forecasts in this manner is meaningful, as a fixed relative deviation from the observed quantity can be regarded as more problematic at high incidence levels rather than at very low incidence levels, which would not be accounted for if only considering the relative deviation.\\  
Overall, we would thus say that while there is no dominant overall model type, mechanistic assumptions don't strictly seem to be necessary for forecasting COVID-19.\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Discuss decomposition%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
As explained in subsection \ref{ssub:weighted_interval_score}, the weighted interval score can straightforwardly be decomposed into penalties accrued from overprediction, underprediction and dispersion. A question that thus naturally arises is whether the score differences discussed previously can be explained by model types performing particularly well or not with respect to a certain component. To this end, we show the decomposed average WIS scores obtained by each model type in Figure \ref{fig:decomp_model_types}. Note that these are the raw average scores rather than scores obtained relative to the baseline model. This is possible since forecasts from all model types are available for the entire time period under study and we first averaged by forecast date and then by model type\footnote{arggh, not actually true. Statistical models missing for the first few weeks in CZ, FR, GB. Could put figure in appendix showing robustness}. We hence see that scores vary substantially more between the different forecast horizons than they do between the model types, but that we can nevertheless observe some interesting facts. 
- Lastly, we also see a general result from previous studies: cases get increasingly harder to forecast than deaths at higher forecast horizons.\\
- For both targets, forecasts from semi-mechanistic models receive higher penalties for overdispersion than the other model types.\\
- For both targets, forecasts from statistical models receive higher penalties for underprediction than the other model types.\\
- relatively bad performance for semi-mechanistic models at longer horizons (while higher overdispersion throughtout, semi-mechanistic models additionally accrue high scores for overprediction at longer forecast horizons.)\\
- for death forecasts, mechanistic models have higher overprediction component\\
- one could of course argue that 4-week ahead predictions are relatively unreliable anyway \\
- consulting overprediction and bias: here we see the ``problem'' with relying on only the overprediction component of the WIS: while semi-mechanistic models get very large scores for overpredicting at longer horizons, the bias metric reveals, suggesting, together with Figure \ref{fig:pw_comp_modeltypes_byperiod}, that the large overprediction score is mostly accrued due to overshooting 
Interestingly, statistical models more commonly receive penalties for underprediction. It's interesting here to mention that through the multiplicative nature of the underlying epidemiological process, it is often "safer" to score models in terms of underprediction than overprediction in terms of absolute WIS scores.\footnote{We want to pull attention to forthcoming work by Nikos Bosse, who investigates scoring forecasts in terms of relative rather than absolute errors.}  \todos{(remove this, as the thought is definitely inspired by Nikos' paper and I can't cite it?)}\\
Furthermore, Figure \ref{fig:decomp_model_types} shows that semi-mechanistic models tend to be better calibrated, that is, show closer to nominal coverage rates than other model types - this is especially true at the $90\%$ level for both targets, and at the $50\%$ level for case forecasts, while these models are slighty underconfident for death forecasts at the $50\%$ level, where the group of statistical models is almost perfectly calibrated. This shows a common trade-off: as shown by the decomposition of the WIS, semi-mechanistic models lack sharpness, but this seems to allow them better calibration, while other model groups issue sharper forecasts at the expense of not containing as many of the observations as they are meant to. As stated in \cite{sherratt_european_2022}, the Hub ensemble suffers from increased overconfidence for Cases with rising forecast horizon on top of general sub-nominal coverage levels, which suggests that these models still might enter favorably into the ensemble by helping to counteract that overconfidence. Whether this actually holds true, we will investigate further in section XX. Generally, we however see a trend of overconfidence for all model types for both targets and furthermore we see a bit of a downward trend in the coverage ratio with rising horizon, especially for case forecasts - this is in line with the results for models in \cite{sherratt_european_2022}. \\
This is again likely due to the case that ``this group on average'' overpredicts in large absolute terms, particularly in period 5, but in general seems to be somewhat well calibrated (?)...\\
Of course, all of this comes with a caveat. Not really a consistent pattern. Sample size not huge. However, we think that this showed that we can't say that one model type is particularly dominating over another and that in particular, it does not seem that one necessarily needs mechanistic assumptions to model COVID-19.\\
The only exception seems to be period 3, mostly driven by Great Britain.\\ 
%%%%%%%
Limitations: groups are quite small, results somewhat diverge. We are doing this since we are interesting in whether or not one can use this to leverage for the ensemble.
AS SCORES ARE HEAVILY INFLUENCED BY LARGER ABSOLUTE SCORES AND THUS DOMINATED BY LOCATIONS WITH HIGHER SCORES; AS WELL AS PERIODS WITH HIGHER SCORES AND LARGER HORIZONS; WE ALSO CONSIDER STRATIFICATION:
\begin{figure}
\centering
\includegraphics[width = \textwidth]{../plots/pw_comp_model_types_across_periods_and_loc_wide.pdf}
\caption{Pairwise comparison of chosen model types across locations. The code to produce this plot was adapted from package \texttt{scoringutils}}
\label{fig:pw_comp_modeltypes_byloc}
\end{figure}
\begin{figure}
\centering
\includegraphics[width = 0.95\textwidth]{../plots/overall_assessment_model_types.pdf}
\caption{Figure showing performance with respect to several scoring rules of the three dominant model types (mechanistic, semi-mechanistic, statistical) in the European Forecast Hub, across five locations over the time period March 2021 - January 2022, for two targets (case and death forecasts) and forecast horizons one to four weeks into the future. Respectively, the depicted scoring rules are: (I), (II) Decomposition of the weighted interval score into overprediction, underprediction, dispersion. (III) Empirical coverage of the $50\%$ prediction interval. (IV) Empirical coverage of the $90\%$ prediction interval. (V) Forecast bias, which ranges between -1 and 1. (VI) absolute error of the median forecasts. Wherever applicable, the desired target value of the score is shown as a black solid line - for all other scoring rules, a lower value of the score corresponds to better performance. Plot heavily inspired by \cite{bosse_comparing_2021-1}. \todos{replace line plots with box plots}}
\label{fig:decomp_model_types}
\end{figure}
\subsection{Model Similarity}
We now turn to the issue of model similarity in ensembles. As expanded upon in Section \todos{XX}, ensemble models are widely regarded to be successful due to the fact that they counteract/mitigate individual model biases and furthermore reduce variance by aggregating a number of models. Regarding the first point of mitigating bias, it is thus conceivable that ensembling approaches could be less successful if some of the included models are too similar. To illustrate this, recall the , thereby skewing 
%include plot of scaled model similarity
%plot of model performance with respect to number of models kicked out
%analyze which models are actually similar 
This notion has some mention (\todos{find better word}) in the literature. For example, in \cite{bosse_comparing_2021-1}, the authors mention that they purposefully did not submit one of their models for inclusion in the forecast hub's ensemble, as there was concern that it could be too similar to another model they already submitted. However, this decision based on the two models' similarity in modeling setup (shortly explain), rather than on an actual judgment of how close their predictions were. Nevertheless, they did find that both models improved the ensemble if included \todos{(find out if this was actually true)}. We now want to do a more systematic review of this concept - since we will consider more models across more countries, we hope to get a more accurate picture.\\
\begin{figure}
\centering
\includegraphics[width = 0.95\textwidth]{../plots/model_similarity.pdf}
\end{figure}
\newpage
\subsection{Ensemble Techniques} \label{sub:ensemble_techniques}
For the mean ensemble, each quantile of the ensemble forecast is simply computed as the mean of that quantile from the component forecasts, that is
\begin{equation}
q^E_{t,h,\tau} = \sum_m^{M_t} w_m \cdot q_{m, t, h, \tau}.
\end{equation}
For an equally weighted mean ensemble, the weights are simply set to $w_i = \frac{1}{M_t}$ for all models, where $M_t$ is the number of models predicting at the given location and target type at time $t$.  \\
We define the weighted median as 
\begin{equation}
q^E_{t,h,\tau} = \text{min}\left\{q_{m^{*},t,h,\tau} \in Q^P_{t,h,\tau}: \sum_m^{M_t}  w_m \mathbb{1}\{q_{m,t,h,\tau} < q_{m^{*},t,h,\tau} \} = 0.5 \ \right\},
\end{equation}
where $Q^P_{t,h,\tau}$ denotes the pool of quantiles for level $\tau$ at time $t$ and horizon $h$ from all available component forecasters. The weighted median is thus the value for which the associated weight of all values smaller than it is half of the total weight. As we restrict $\sum_m^{M_t}w_m = 1$. Similarly to above, if we set $w_m = \frac{1}{M_t}$ for all models, we obtain the normal median. If this value is not exactly attained within the pool of quantiles, linear interpolation is performed. For our purposes, we used the \texttt{weighted.median} function from the \texttt{spatstats} package \citep{baddeley_spatstat_2005}.\\
A natural follow-up question is thus how to determine the weights 
- weighted, different types of weight computes: QRA, inverse score weighting, inverse score weighting with exponential smoothing. \\
Theoretically, one could also determine weights separately by horizon - however, this has the danger of \\
Weighting by past performance: the most natural choice as a score is again the weighted interval score, as it is the most parsimonious way to score forecasts in terms of both calibration and sharpness. 
\subsection{Adding a new model} \label{sub:adding_a_new_model}
We thus designed the following experiment: for each forecast date, we first built every possible ensemble of size $n$ from the available models at that date. The problem with investigating ensemble size as it relates to performance in the dataset is that, as amply mentioned beforehand, the actual realized ensemble size is not constant as models drop in and out of the Hub, while we also have no universal measure of absolute forecast skill, and one would need to disentangle ensemble size from the inherent level of difficulty present at a given forecast date. In a regression, one could of course control for these idiosyncrasies, but as also mentioned beforehand, scores are not suited as the dependent outcome in a regression and we are furthermore left with the problem of not having a lot of data. But, if we have data on the average score of an ensemble of size $n$, we can study the (expected) added benefit of adding another model to the ensemble. This has some interesting dimensions that we want to explore:\\
- are there times (high vs. low incidence is what most easily comes to mind) where adding a model is more beneficial?\\
- is adding a model more beneficial for the mean or the median ensemble? \cite{bosse_comparing_2021-1} found that performance changes from adding or removing a model from the ensemble were ``of a similar order of magnitude, suggesting that at least in this instance, with a relatively small ensemble size, the median ensemble was not necessarily more ``robust'' to changes than the mean ensemble''. They find also that adding a 
- are there differences between adding a model that is more vs less distant to the current ensemble?
- how much does individual model performance matter? How "safe" is it to add any model, even if it might be a bad performer? We deliberately chose the ``perfect knowledge situation'' where we knew about actual current model performance. 
- are there models that if added provide a particularly good benefit?\\
We see that adding a very close model has almost no impact on the mean ensemble, while it does move the median ensemble. Since the median is a more discrete measure, its predictions are more skewed by adding another model. \\
Of course, this has the downside of limiting the maximum ensemble size that can be studied. This can better enhance understanding of ensemble behavior. To our knowledge, such an experiment is novel in the literature. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%e
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ensemble Experiments}
Some of these techniques came up . We thus retroactively analyze other ensembling methods. In light of the fact that . One could in theory denote a holdout set, but given the fast changes in disease characteristics, the periods are not really comparable.\\ 
While this is a retroactive analysis, it is generally important that it, to the extent that it is possible, simulates the real-time. That is, only information that is available up to that point in time may be used for building the ensemble. This is because we want to trial (and if successful, suggest) methods that in theory could be deployed in real-time. Otherwise, it would be too easy to retroactively devise strategies that outperform the ensemble.\\
Generally, for all "ensemble experiments", the sets of models that would enter the ensemble were automatically stratified by target type, location and of course the forecast date. Theoretically, one could also stratify by horizon.\footnote{Or even quantile? As done by US hub in ensemble experiments.} The reasoning for this is clear: some models or model types might be better at forecasting certain horizons than others. However, this could lead to things going entirely haywire \\
Note that we refrained from excessively tuning hyperparameters in the study.
We wanted to derive methods that accounted for the fact that weight estimation does not work well in low signal-to-noise ratio settings \cite{claeskens_forecast_2016}.\\
We decided to implement an ensemble that is based on the results from 1, i.e. that namely excludes semi-mechanistic models at longer horizons. Of course, we also saw that mechanistic models had rather poor performance at longer horizons, but we decided to exclude only semi-mechanistic as we otherwise wouldn't have much of an ensemble left at all. Furthermore, while the trend isn't $100\%$ clear, we mostly see that semi-mechanistic models perform either second-worst or worst. Also, while we identified a general tendency for larger dispersion (which might actually aid the generally overconfident ensemble), we additionally at larger horizons saw a much more problematic tendency to overpredict the target. We will do this analysis for both cases and deaths, but of course expect larger benefits for case forecasts. Compared with both the relative scores of the other model types, we see that semi-mechanistic models are rarely a good performer when compared to the other model types.  We of course expect this to improve performance in-sample, but further we want to investigate whether this introduces any unwanted effects, i.e. discontinuities. Namely, there are a few desirable properties of forecasts that could be in jeopardy if the set of models in the ensemble varies with the forecast horizon. It is thus conceivable that these models increase the cone of uncertainty for lower horizons, making it drop afterwards if they are then excluded. Furthermore, we also monitor the resultant coverage of the ensemble before and after, to see if anything worsened in this regard - wis might drop (not a surprise, as this is how it was selected), but that makes monitoring other scoring rules even more important. \\
For evaluation, we use a more direct relative WIS, since there are no missing forecasts in this part of the analysis.\\
Our analyses in the previous chapters showed some (if slight) performance differences of  \\
Lastly, we chose to compare our methods to an ensemble that employed the unweighted median over the entire study period rather than the actualized Hub ensemble. This is due to the aforementioned fact that the Hub changed their aggregation method from mean to median in July(?) of 2021. We noticed that in the aggregate (that is, over the entire study period), some of our methods, most notably those based on the weighted median, compared favorably to the actualized Hub ensemble, which however mostly stemmed from the early months of 2021. We felt that this was somehow misleading, so instead opted to use the median ensemble as a benchmark throughout the study period. We checked that our ensembling method gave similar predictions to the hub ensemble. Whenever we refer to the ``benchmark'' model during this section, we are thus referring to the equally weighted median ensemble that indiscriminately includes all models. Whenever we refer to aggregate performance, we refer to .... \\
%\subsection{Model Types}
%These compartmental models, via a set of differential equations, explicitly model how members of the population transition through the states of being susceptible, (exposed), infected, and recovered/removed \cite{taylor_combining_2021}.
\newpage
\subsection{Best performers}
In this section, we investigate whether changing the model base such that only the recent best performers are included in the ensemble, potentially in conjunction with additionally weighting component forecasts based on past performance, can improve performance relative to the equally weighted median ensemble that indiscriminately includes all models. We have several reasons why we believe this approach could prove fruitful. \medskip\\
%%%%%%
First of all, this approach has some precedence in the U.S. Hub: \cite{ray_comparing_2022} find that a combination method that simultaneously reduces the model set to the ten recent best performers and subsequently weights these by their past performance (in terms of the relative WIS) performs consistently better than an equally weighted median for forecasting deaths, while it does not work better for forecasting cases. As a consequence, the relative WIS weighted median ensemble superseded the equally weighted median ensemble as the U.S. Hub's official method for forecasting deaths in November 2021.\\ 
In fact, they state that they also applied this method to the European Hub, where the method however showed worse performance relative to the equally weighted median including all component forecasts, for both deaths and cases. This could however be due to the substantial differences in model availability between the two Hubs - as previously stated, most models participating in the U.S. effort submit forecast for all locations (that is, states), while this is only the case for less than half the models in the European Hub. Since their method only selects the best ten models measured by relative WIS across the entire European set, we surmise that it could fail due to two reasons.\\
First of all, the approach in effect substantially reduces the model set for most locations, as it will likely select some models which only submit forecasts for a small number of locations or even just a single location - recall our finding in section \ref{sub:model_types_analysis} that found that Poland's agent-based models had substantially lower relative WIS than other model types, at least for forecasting cases. This way of selecting models presumably already leads to worse performance due to the sheer effect of greatly reducing ensemble size at some locations, and potential benefits of the method would thus not get a chance to properly unfold.\\
Second of all, we noticed during our investigations of the data that a model that is a good performer for one location at a given time isn't necessarily one for all other locations. Moreover, when using the WIS to judge component forecasts, we could have the case that locations with nominally high levels of incidence could, in a sense, dictate the model set for all locations. We thus believe that it's better to allow stratification by location, even though it could also make the approach more vulnerable to variations in single models' performance. \\
Hence, we will investigate whether an approach of treating each location's model set separately can improve performance, as it ensures that each location is given a full set of best $k$ performers unique to that location. Nevertheless, we are still dealing with the aforementioned issue that the model base is not large to begin with, so even when attempting to draw from the available resources as much as possible, reducing ensemble size further could counteract any potential benefits from choosing better performers, as the ensemble in theory becomes more vulnerable to failure or erratic behavior of single component forecasts if the number of models is small. \medskip\\%Fundamentally, the better we are  This is due to the fact that individual performance can be erratic and past performance is not a perfect predictor to recent performance. \\
Second of all, \cite{sherratt_european_2022} showed that for the European Hub, employing a weighting scheme where all models are weighted based on their past performance did not provide a benefit over the equally weighted median, neither when subsequently using a weighted mean or weighted median. The question thus remains whether either pure model selection or combining weighting with prior selection of recent good performers could provide a benefit. In the case where it is possible to identify models that truly have an edge over others, it might be beneficial to reduce the model set to these models, whether or not they are subsequently weighted, as estimating weights for all models could be too noisy.\\ %We however don't expect good results from weighting, as we believe that estimating weights in a reduced model set could actually be detrimental in a situation where noise is high (model performance generally varies a lot). We rather expect a strict reduction of the model set, which effectively amounts to setting all other weights to zero, to perform better. The rationale behind this approach is that it is to a certain degree less flexible than choosing true weights, i.e. a sort of shrinkage method (similar to LASSO). However, this could only work if consistent performers exist and they can reliably be identified. \todos{maybe better: just give arguments for both approaches without saying which one is better}. Considering just selection or selection with weighting: in case there is signal and not just noise it might be good to weight.\\
Finally, note that here the procedure is different to what we did in section \ref{sub:adding_a_new_model}: we are actively removing models from the set, while before we investigated the effect of adding a model to the ensemble. While these are to a certain degree just two sides of the same coin, in practice - due to the expected gains in ensemble performance when adding any model - an ensemble builder is more realistically faced with the decision of removing a model from the existing model base rather than adding an extra model to it. \medskip \\
We thus proceeded as follows. At each forecast date, via a rolling window of the respective past four weeks, we select the number of $k$ models with lowest relative WIS within that window. If one of the selected models did not participate at the current date, the next best model takes its place - put differently, we only select from the set of models that are actually available at the current date, to ensure that ensemble size is always constant. Forecasts that have not resolved yet by the respective forecast date are excluded from scoring, since the resolution of, for instance, the most recent 3-week forecast would not be in the information set of the ensemble forecaster in a real-time setting, which we want to mimic as closely as possible. The set of scored targets thus includes one 4-week ahead forecast, two 3-week ahead forecasts, three 2-week ahead forecasts and four 1-week ahead forecasts.\\
In principle, the window size is of course a tuning parameter that can affect the results. Given that our analysis is entirely retrospective, we refrained from excessively tuning this parameter to the point where it would give the most favorable in-sample results. We therefore chose a similar value to the one used by \cite{bracher_evaluating_2021} in a related analysis - they used a window size of three weeks to estimate weights, which we slightly increased to four weeks as we wanted to include at least one forecast from all horizons. In theory, this parameter can be seen as a dial between the bias and variance of estimating model weights: in so far as average model performance is not stationary, bias decreases if we use less recent observations, while variance increases (due to random fluctuations in model performance having more effect on the weights). We believe that four weeks strikes a good balance between the two. \\%We however also mention that in practice, \cite{ray_comparing_2022}, who use a window of 12 weeks, state that they did not observe substantial differences when varying this parameter.\\
%For this approach, we scored models based on their performance in the past four weeks. To assess this performance, we only scored forecasts that had resolved by the current forecast date, in order to better mimic the real-time situation. This means that we excluded forecasts if they were made before the current date, but for a horizon that as yet lay in the future relative to the current date. \\ 
To mitigate the recently mentioned issue of varying model base, we score models and thus construct the best performers set separately for each location, as well as separately for the two series. Within the window of four weeks, we allowed models at most one missing forecast, otherwise they were not considered for selection. This is the reason we chose to use the relative WIS rather than the average score obtained by each model, since, as amply mentioned beforehand, the average score can be heavily influenced by single targets that were particularly hard to forecast and thus induced large absolute scores. Models that missed forecasting this particular target could thus receive a relatively low average score and as a consequence undeservedly be included in the best performers set.\\
As an aggregation method for the thereby induced sets of best recent performers, we considered the equally weighted median, the equally weighted mean, as well as a weighted approach for both. For the weighted methods, we employed inverse score weighting as detailed in section \ref{sub:ensemble_techniques}. In the cases where component forecasts had missed one target within the current window used to estimate the weights, we proceeded as in \cite{bracher_evaluating_2021} and imputed that past score with the worst score achieved by any model for that particular target.\\
%equally weighted median ensemble constructed from the full available model base
\begin{figure}
\centering
\includegraphics[width = 0.95\textwidth]{../plots/best_performers_boxplot}
\caption{\footnotesize{Boxplots showing the spread of WIS scores of the selection ensemble methods (based on recent component forecast performance) relative to the benchmark (equally weighted median ensemble including all models), by location and number of best recent performers ($k$) included. The four panels show the resulting spreads when subsequently applying different ensemble methods to the selected component forecasts: The first panel (I) shows the results for the equally weighted median, the second panel (II) for the equally weighted mean, the third panel (III) for the inverse score weighted median and the fourth panel (IV) for the inverse score weighted mean. For all methods, selection of component forecasters and (if applicable) estimation of weights is based on performance in the past four weeks. Outliers with relative scores over 3 were excluded from the plots for legibility - to compensate for this, the diamond-shaped black points represent the mean relative score value. Values below one mean that the respective method outperforming the benchmark for the given target. Missing boxplots for a given $k$ correspond to the cases where the respective combination of location and target type did not have a sufficient model base to support the analysis.}}
\label{ref:best_perform_boxplot}
\end{figure}
We varied the number of component forecasts included, namely, we tried $k = (3,5,8,10)$. For a given $k$, we excluded a location and target type combination if the size of its model base was smaller than $(k+2)$ for at least half of the study period. We did this as we believed that in these cases the analysis would conceptionally be too similar to an ensemble of all models (that is, there is no meaningful choice of ``best'' performers if almost all models are included over most of the study period) and could therefore portray the method more favorably than it is in actuality. Along the same lines, we considered accounting for the fact that the model base was generally smaller during the winter months, where the approach in principle was accordingly more similar to the benchmark. We however saw no meaningful differences in the results across different periods, so we report the results aggregated over the entire study period.\\
\begin{table}[t]
\centering
\begin{tabular}{lll}
\hline
location & target\_type & models\\
\hline
CZ & Cases & ILM-EKF (35), IEM\_Health-CovidProject (31), MUNI\_DMS-SEIAR (28)\\[0.1em]
DE & Cases & itwm-dSEIR (25), ILM-EKF (20), ITWW-county\_repro (16)\\[0.1em]
FR & Cases & ILM-EKF (34), USC-SIkJalpha (32), IEM\_Health-CovidProject (31)\\[0.1em]
GB & Cases & IEM\_Health-CovidProject (34), MUNI-ARIMA (27), ILM-EKF (23)\\[0.1em]
PL & Cases & MOCOS-agent1 (29), epiforecasts-EpiNow2 (23), ILM-EKF (21)\\[0.8em]
CZ & Deaths & UMass-MechBayes (36), ILM-EKF (28), epiforecasts-EpiNow2 (24)\\[0.1em]
DE & Deaths & HZI-AgeExtendedSEIR (28), RobertWalraven-ESG (18), USC-SIkJalpha (18)\\[0.1em]
FR & Deaths & RobertWalraven-ESG (32), UMass-MechBayes (32),\\
& & IEM\_Health-CovidProject (31), ILM-EKF (31)\\[0.1em]
GB & Deaths & IEM\_Health-CovidProject (26), USC-SIkJalpha (26), LANL-GrowthRate (21), \\
& & RobertWalraven-ESG (21), UMass-MechBayes (21)\\[0.1em]
PL & Deaths & MOCOS-agent1 (37), UMass-MechBayes (29), ILM-EKF (27)\\
\hline
\end{tabular}
\caption{Table showing models chosen}
\label{tab:bp_chosenmods}
\end{table}
Before we turn to the results in performance, we want to shortly pull attention to Table \ref{tab:bp_chosenmods}, which, for $k = 5$, displays each location's top three chosen models using the method, separately for Cases and Deaths. We do observe that there are some models (most prominently \texttt{ILM-EKF} for Cases and \texttt{UMass-MechBayes} for Deaths) that are chosen across most or all locations. However, we also both see models (such as \texttt{epiforecasts-EpiNow2} or \texttt{MUNI-ARIMA}) that in principle submit forecasts for all locations, but are more often chosen at some locations than others, as well as some models, such as \texttt{ITWW-county-repro} or \texttt{MOCOS-agent1} that forecast for only one or two locations. Recall that we argued that the U.S. approach could be fundamentally flawed for an application in the European Hub as it does not account for the fact that models could have considerable variations in performance across locations. While it does seem to prove true that there are considerable performance differences across locations and a more flexible method will accordingly choose different models, as we will shortly see, accounting for this is not enough to gain competitive performance in the European Hub.\medskip\\
% As we will shortly see, but we thereby argue that we have ruled out the lack in flexibility in model choosing induced by the aforementioned U.S. approach as the sole cause of the fact that the method does not work all too well in the European Hub. Thus, even though we, contrary to the U.S. Hub approach, addressed both the lack in flexibility and the number of models , thereby ruling these out as the sole causes of the issue.\\
%- this is just to demonstrate that the reason for bad performance is not only due to the inflexibility in model selection.\\
%a. we see that e.g. mocosagent would have reduced the ensemble size.
%b. \\
\begin{table}[t]
\centering
\begin{tabular}{lrlrrrrrr}
\hline
model & nmod & target\_type & Average & CZ & DE & FR & GB & PL\\
\hline
unw. median & 5 & Cases & 1.10 & 1.06 & 1.57 & 1.02 & 1.05 & 0.70\\
weighted median & 5 & Cases & 1.03 & 0.87 & 1.23 & 0.99 & 1.09 & 0.65\\[0.4em]
unw. mean & 5 & Cases & 1.18 & 1.20 & 1.33 & 1.17 & 1.18 & 0.92\\
weighted mean & 5 & Cases & 1.18 & 1.17 & 1.51 & 1.10 & 1.18 & 0.90\\[0.4em]
unw. median & 10 & Cases & 0.98 & - & 1.01 & - & - & 0.91\\
weighted median & 10 & Cases & 0.97 & - & 1.05 & - & - & 0.78\\[0.4em]
unw. mean & 10 & Cases & 1.01 & - & 0.95 & - & - & 1.14\\
weighted mean & 10 & Cases & 1.11 & - & 1.16 & - & - & 1.02\\[0.4em]
unw. median & 5 & Deaths & 1.29 & 1.10 & 1.36 & 0.98 & 1.24 & 1.56\\
weighted median & 5 & Deaths & 1.13 & 0.93 & 1.29 & 0.84 & 1.19 & 1.28\\[0.4em]
unw. mean & 5 & Deaths & 1.45 & 0.99 & 1.68 & 1.42 & 1.19 & 1.65\\
weighted mean & 5 & Deaths & 1.41 & 0.97 & 1.73 & 1.21 & 1.19 & 1.63\\[0.4em]
unw. median & 10 & Deaths & 1.08 & - & 1.10 & - & - & 1.06\\
weighted median & 10 & Deaths & 1.07 & - & 1.12 & - & - & 1.03\\[0.4em]
unw. mean & 10 & Deaths & 1.36 & - & 1.35 & - & - & 1.36\\
weighted mean & 10 & Deaths & 1.47 & - & 1.46 & - & - & 1.48\\
\hline
\end{tabular}
\caption{Blabla}
\label{tab:rel_wis_best_performers}
\end{table}
We now turn to an actual discussion of the results. The aggregated results can be seen in Table \ref{tab:rel_wis_best_performers}, which shows the ratio of the average WIS obtained by the respective method and that of the equally weighted median ensemble including all available models. We furthermore show the variation of relative WIS scores obtained at each target (i.e., forecast date), stratified by location and target type, in the boxplots in Figure \ref{ref:best_perform_boxplot}. For both, we additionally included the relative scores that result from averaging across all locations.\\
For the ``average location'', we can directly see that for all numbers of component forecasters included in the best performers set, scores are consistently either similar to or worse than for the equally weighted median ensemble including all models. Choosing a subset of best performers thus does not seem to be a viable alternative to the benchmark, whether or not the models are subsequently weighted. Nevertheless, we want to discuss some interesting trends and results, as well as attempt to establish reasons/heuristics for why the approach might fail.\\
As a general trend, we see that all approaches tend to work better the more models are included: Figure \ref{ref:best_perform_boxplot} shows that variability in performance generally decreases and the median and average relative scores generally move closer to 1 with increasing $k$. This suggests that the positive effect of ensemble size outweighs that of selecting models - the big exception here is the cases series for Poland, which we will discuss separately further down. In other words, the closer the setting is to an ensemble that indiscriminately includes all component models, the better, whether or not the models are subsequently weighted. However, we again make not of the fact that our model base is very small. It could be the case that there exists a critical threshold of number of models, after which the added marginal benefit of adding another model is smaller and benefits from model selection can actually be realized. If this were the case, we could simply still be sitting firmly in the region of ``more models are always better''.\\
%We thus expect that it is likely the case that one either needs  there exists a critical threshold for ensemble size that one needs to clear before selection makes sense, as this will give a larger pool of models to choose from. \\ in lieu of the existence of some actual top performers. It could thus be that for the locations (with the exception of some actual top performers) are still in a range of "more models are always better"\\
Furthermore, we see that the mean approaches, whether weighted or not, generally see a higher average relative score, again highlighting the general vulnerability of the mean ensemble to outlying forecasts and thereby the benefit of using the more robust median ensemble, especially when working with a small number of component forecasts. 
%furthermore supporting the claim that the mean ensemble can undesirably produce outlying forecasts and is not as robust, especially when model base is small
Moreover, while additional weighting seems to either not impact or improve the median ensemble, it sometimes has large negative effect on the average score of the mean ensemble, for instance for the Case series in Germany for $k = 5$ - if we look at the boxplots, it becomes clear that this mostly stems from large outlying relative scores. This is likely due to the fact that the mean ensemble is already more vulnerable to deteriorations in performance from one of its member forecasts, especially in small model sets - if such a forecast is additionally given a higher weight due to previous good performance, we can expect performance to further decline in such a situation.\\
Thus, since both the weighted and unweighted mean compared unfavorably, we generally focus our remaining discussion on the respective median approaches. \medskip\\
The only case where we see a slight improvement of scores for the average location relative to the benchmark is for forecasting Cases and $k = 10$ - looking at the locations' individual results, we immediately see that this is only due to the results from Poland, where performance is actually improved across all $k$, especially for the weighted median ensemble. We now want to investigate why this is the case, exemplary for $k = 10$ and in contrast to the Cases series from Germany. Afterwards, we also turn to a short discussion about forecasting Deaths.\\
Thus, consider Figures \ref{fig:bpweights_de} and \ref{fig:bpweights_pl}, where we display the weights given over time to the five models chosen most often, as well as those models' relative WIS scores, their 2-week ahead predictions and the WIS score of the weighted median relative to the benchmark, for Poland and Germany, respectively.\\
\begin{figure}
\centering
\includegraphics[width = 0.9\textwidth]{../plots/best_performers_weights_de}
\caption{\footnotesize{Performance of weekly case forecasts for Germany, for the weighted median selection ensemble based on the ten component forecast with best performance, and its component forecasts. Component forecasts are selected and weighted based on their performance in the most recent four weeks. Across all plots, the five models chosen most often for the selection ensemble are highlighted. Panel (I) shows the relative performance of the weighted median selection ensemble based on recent component forecast performance, relative to the benchmark (unweighted median ensemble including all models). Panel (II) shows two week ahead predictions of all component forecasts, as well as the observed values of the series. Panel (III) shows performance of component forecasts over time, in terms of relative WIS. Panel (IV) shows the weekly distribution of weights given to component forecasts. Note that as forecasts are scored by the forecast date, performance measure series lead ahead of the observed incidence series. Plot inspired by \cite{ray_comparing_2022}.}}
\label{fig:bpweights_de}
\end{figure}
In the case of Germany in Figure \ref{fig:bpweights_de}, we can nicely see the fundamental issue of selecting and/or weighting models based on recent performance, which is the potential non-stationarity of model's relative performance and the thereby induced unreliability of using past performance as a predictor for current performance. We will now walk the reader through these plots to illustrate that issue.\footnote{At this point, we want to again explicitly mention an important characteristic of the way forecasts are scored here, to clarify the way these plots need to be interpreted. We score models by the forecast date, not by the target date. That is, if a model issues a 3-week forecast on May 3 that severely overshoots the target as it realized on May 22, this will be reflected on its score on May 3. Thus, if models for instance overshoot at longer horizons, this is reflected in the scores \textit{before} the target date.} Throughout the summer months of 2021, the weighted method performed comparatively to the benchmark, as it could identify component forecasts that - to a certain extent - had consistent good performance relative to other forecasters. As autumn approaches, it has placed close to half of its weight on four of these component forecasts. It is at this time that cases start rising and (especially at longer horizons), these models overshoot the target. Critically, they overshoot the target more than alternative models: while other models mostly exhibit similar behavior and thus also compare unfavorably to the baseline model of ``no change'', they still stay closer to the target and the ensemble would thus have fared better if it had drawn from its entire model base and not placed undue weight on a small number of component forecasts - as a consequence, the benchmark (which includes all models) performs better than the weighted method during this time.\\ 
Thereafter, during the month of September, these models are sequentially dropped from the set of best performers and hence given zero weight, but only after they have influenced and thus worsened the performance of the ensemble. Moreover, we can see the same behavior again during December of 2021, where models that have received higher weight subsequently overshoot the target and hence undermine the ensemble's performance - the most notable example in this regard is perhaps \texttt{FIAS\_FZJ-Epi1Ger}, which receives especially high relative WIS scores during these moments of ``collective overshooting'' and is consequentially most often dropped altogether from the selected ensemble.\\ 
Thus, while the method stays somewhat close to benchmark performance when averaging over all forecast dates (at a relative score of 1.05), we can see that it undesirably fails in critical times, making it altogether an unattractive alternative to the benchmark.\medskip\\
%Consider, for instance, the model \texttt{FIASFZJ-Epi1Ger} - throughout the summer months of 2021, it performs relatively well and is awarded positive weight in the ensemble.  during the late fall period of 2021 it issues fjust as its performance recovers during the fall and it is again given positive weight, its performance deteriorates again.\\
\begin{figure}
\centering
\includegraphics[width = 0.9\textwidth]{../plots/best_performers_weights_pl}
\caption{\footnotesize{Performance of weekly case forecasts for Poland, for the weighted median selection ensemble based on the ten component forecast with best performance, and its component forecasts. Component forecasts are selected and weighted based on their performance in the most recent four weeks. Across all plots, the five models chosen most often for the selection ensemble are highlighted. Panel (I) shows the relative performance of the weighted median selection ensemble based on recent component forecast performance, relative to the benchmark (unweighted median ensemble including all models). Panel (II) shows two week ahead predictions of all component forecasts, as well as the observed values of the series. Panel (III) shows performance of component forecasts over time, in terms of relative WIS. Panel (IV) shows the weekly distribution of weights given to component forecasts. Note that as forecasts are scored by the forecast date, performance measure series lead ahead of the observed incidence series. Furthermore, note that the y-axis of panel (I) was deliberately stretched for compatibility with panel (I) in Figure \ref{fig:bpweights_de}. Plot inspired by \cite{ray_comparing_2022}.}}
\label{fig:bpweights_pl}
\end{figure}
In contrast to this, we now discuss the Case series for Poland, where, as mentioned, the weighted selection ensemble performs substantially better in the aggregate. At the beginning of the study period, the selection ensemble places a large proportion of its total weight on three performers (\texttt{MOCOS-agent1}, \texttt{ILM-EKF} and \texttt{epiforecasts-EpiNow2}) that perform relatively well at predicting the decline in case numbers. As a consequence, the weighted selection ensemble compares quite favorably to the benchmark. Subsequently, during the summer and fall months, model performance is overall more similar and relative performance stays mostly close to one - \todos{find out why it performs a little worse at times }% we would argue that the selection method mostly performs a little worse here compared to the benchmark due to the fact that it effectively excludes a large number of models. 
But perhaps most importantly, heading into the winter months of 2021, where Poland (similar to Germany and some other countries in the set) experiences a sharp rise in cases followed by a decline and subsequently another sharp rise, it manages to more heavily weight some models that exhibit relatively less overshooting behavior, thereby overall performing better than the benchmark. \\We would also argue that the behavior near these critical times is actually more meaningful than that during the summer months which overall have very low incidence rates and are thus not of the same level of interest to decision makers. To conclude the discussion for Poland, we would thus argue that Poland contains models (most notably the agent-based \texttt{MOCOS-agent1}, which only predicts in Poland) that seem to exhibit particular foresight when forecasting cases - it thus makes sense to more heavily rely on these models. \\%In lieu of the existence of these models, it doesn't really make sense.\\ 
However, there is of course no guarantee that this behavior could be replicated for the same series during other critical times. Furthermore, for the other countries in the set, we on the one hand also obtain a higher relative skill score in the aggregate compared to the benchmark and overall observe similar trends to that exhibited by Germany \todos{(more appendix plots?)}. Thus, we argue that a central problem with the selection methods is that, as a model might exhibit good performance over a given time period and is thus given nonzero or higher weight in the ensemble, the selection ensemble increasingly makes itself vulnerable to performance fluctuations of such a model.\\ 
Specifically for the Cases series in the U.S. Hub, \cite{ray_comparing_2022} also state that they have observed similar behavior: they give the explanation that the best performing component models for forecasting Cases were generally those that were observed to extrapolate recent trends. This would mean that, as trends change from mostly level to increasing, these models would actually cease to be the ``best'' models, as extrapolating recent trends is fundamentally associated with overshooting peaks. This is in and of itself an undesirable behavior, but in addition (due to the higher level of the series) also gives higher absolute scores to those forecasts as a consequence.\\
%the ``best'' forecasting models for Cases would also change from those . 
However, precisely as these changes in trends happens, the weighted selection ensemble has already selected for and placed more weight on models that performed well in recent weeks, thereby making itself more vulnerable to their behavior and excluding models which would have mitigated the trend-extrapolating (i.e., overshooting) behavior. By the time it has adapted to this and accordingly downweighted the overshooting models, the peak will have usually already passed. While we can't claim without fail that this is the root cause of the issue, this explanation is most in line with what we have discussed and similar trends we observed for the other countries in the set.\medskip\\
Pertaining to the Deaths series, our results somewhat diverge from those in the U.S. Hub, where the series sees consistently improved performance from weighted selection. In our case, in the aggregate, weighted selection actually performs slightly worse for forecasting deaths than for forecasting cases, although we again must note that we see somewhat diverging results for the different locations and we therefore can't claim the difference to be meaningful. Nevertheless, we can definitely not claim a consistent improvement as in the U.S. Hub.\\
As an exemplary illustration for the Deaths series, we now perform the same analysis for the United Kingdom and $k = 5$ in Figure \ref{fig:bpweights_gbdeaths}. In panel (III), we see that overall, most models seem to be similarly good at forecasting deaths - in fact, over- and under-prediction of the target (and thereby fluctuations in performance) seem to be happening at random (i.e., non-predictable) times, which leaves the ensemble usually starting to put weight on a component forecast right as its performance starts to deteriorate again. This is also reflected in the assigned weights, with models usually not staying in the set for prolonged periods. While we thus don't observe performance that is ever as detrimental as that which we described for Germany near peak times, the lack of consistent performance in any component forecasts and the thereby induced failure of the selection method to pick out forecasters at the precise times where they are actually performing well means that the weighted selection method accrues continually slightly higher scores over the entire study period. Thus, the weighted selection ensemble receives a bad score compared to the benchmark averaged over the entire period.\\
\begin{figure}
\centering
\includegraphics[width = 0.9\textwidth]{../plots/best_performers_weights_gbdeaths}
\caption{\footnotesize{Performance of weekly case forecasts for United Kingdom, for the weighted median selection ensemble based on the five component forecast with best performance, and its component forecasts. Component forecasts are selected and weighted based on their performance in the most recent four weeks. Across all plots, the five models chosen most often for the selection ensemble are highlighted. Panel (I) shows the relative performance of the weighted median selection ensemble based on recent component forecast performance, relative to the benchmark (unweighted median ensemble including all models). Panel (II) shows two week ahead predictions of all component forecasts, as well as the observed values of the series. Panel (III) shows performance of component forecasts over time, in terms of relative WIS. Panel (IV) shows the weekly distribution of weights given to component forecasts. Note that as forecasts are scored by the forecast date, performance measure series lead ahead of the observed incidence series. Furthermore, note that the y-axis of panel (I) was deliberately stretched for compatibility with panel (I) in Figure \ref{fig:bpweights_de}. Plot inspired by \cite{ray_comparing_2022}.}}
\label{fig:bpweights_gbdeaths}
\end{figure}
As we discussed in section \ref{sub:hub_data} and as \cite{ray_comparing_2022} also remark in their discussion of the weighted selection ensemble, Deaths are widely considered the easier target to forecast - one reason for this is that the number of deaths most notably depend on the number of past cases, which is an observable quantity \todos{(cite, e.g. Cramer (2022)?}. On the flip side, if all models show about the same level of skill at predicting future death counts, this also means that it is harder for any model to perform relatively well to another, and therefore harder for the selection method to pick out meaningful better performers. \\
%In fact, when we regard the differences in performance with respect to the two series,  for both the mean and median ensemble, we can see that relative scores for cases are slightly lower than for deaths, which is somewhat contradictory to the aforementioned results in the U.S. Hub. 
%Hence, we would argue  this is likely not due to an inherent difference between the two series: 
As \cite{ray_comparing_2022} argue, the trained method likely works well for forecasting deaths in the U.S. Hub simply because there exist forecasters that have a reliable and consistent track record for forecasting that series, while forecast performance for cases tends to be less stationary for all models. Thus, it is most likely the case that such forecast models simply don't exist for the European Hub and we would argue that, contrary to forecasting cases, the approach seems to fail for forecasting deaths due to overall similar model performance rather than due to failure at critical times. \medskip \\
%In general, we also see that cumulative amount of weights given are never that high.\\
%karlen-pypm not pciked that often Again supporting the point that a model that works well in some locations might not work well in others. (however, availability was also low)\\
%make the argument that and that following structural breaks in the series ()
%During both periods, it would often be beneficial for the ensemble to draw on its entire model base, as there are several other models that receive lower scores E
%%%%%%%%%
%"Trained ensembles that estimated weights based on past performance suffered,as they started to upweight those component forecasters just as their performance dropped. This recurring pattern highlights the challenge that nonstationary component forecaster performance presents for trained ensembles".\\
\begin{figure}
\centering
\includegraphics[width = 0.95\textwidth]{../plots/best_performers_coverage.pdf}
\caption{Assessment of probabilistic calibration of the selection ensemble methods based on recent component forecast performance, relative to the benchmark (equally weighted median ensemble including all models). The upper panel (I) shows central interval coverage of the methods, that is, the proportion of observations falling into the predictive distributions' central prediction intervals. The lower panel (II) shows the difference between the empirical and nominal coverage of the predictive quantiles. Negative (positive) values indicate that less (more) observations fell below a predictive distribution's respective quantile than \todos{intended (find better word)}. For both panels, the black dashed line corresponds to optimal calibration. For the selection methods, results are based on selecting five best performers for locations with comparatively small model base (Czech Republic, France, United Kingdom) and ten best performers for locations with comparatively large model base (Poland, Germany).}
\label{fig:bpcoverage}
\end{figure}
To assess the methods' probabilistic calibration, we depict coverage of the central prediction intervals as well as one-sided quantile coverage, separately for the different methods, in Figure \ref{fig:bpcoverage}. For coverage of the central prediction intervals, we see in the upper panel that for forecasting cases, all methods are generally overconfident, that is, the predictive distributions' central intervals overall do not cover as many observations as they should, given the respective nominal coverage level. %The difference between the desired coverage level and the empirical coverage level generally increases with the nominal coverage level, which is to be expected. 
The lower panel suggests that this is more of an issue of the upper rather than the lower quantiles, which indicates that the predictive distributions have some downward bias - this is however more the case for the median-based approaches than the mean-based ones, with the weighted median showing most downward bias out of the methods considered. These results are mostly in line with the results of \cite{ray_comparing_2022} for the European Hub, but in contrast to them, we observe that the weighted mean's quantile coverage is more in the line with that of the unweighted mean than that of the weighted median.\\ 
For forecasting deaths, we observe that empirical coverage of the central prediction intervals is in general closer to nominal coverage, with only slight under-confidence visible for the benchmark and the mean-based ensembles. According to panel (II), the weighted selection median is slightly downward biased, with consistently less observations falling below the respective quantiles than required - this is also reflected in its bias measure (which sits at -0.123). In contrast to this, the mean-based approaches show some slighter upward bias,  while the remaining median-based approaches' empirical quantile coverage is more in line with nominal coverage. These results slightly deviate from those of \cite{ray_comparing_2022} for the European Hub - in particular, they find that the weighted median is over-confident, but not biased. \\ %Furthermore, ensembles based on the median as a summary function in general tend to have lower one-sided rates of quantile coverage, with the weighted median being the worst offender. \\
Overall, the methods are thus mostly similar in terms of probabilistic calibration. However, we must note that while the weighted median tended to perform best out of the selection methods considered with respect to relative WIS scores, it compares less favorably to other methods when assessing calibration, mostly due to its downward bias. \medskip \\% making it a less  option.
For the cases series, we also considered the possibility to focus our analysis (both estimation of weights and the evaluation) on shorter horizons, namely for up to two weeks. This is due to the reason that (as mentioned in section XX) it is a recurrent result across the Hubs that case forecasts tend to be more unreliable for longer horizons and it thus might be desirable to optimize for the one and two week horizons. However, we observed no clear trend that suggested the (weighted) selection method as it was implemented performs better for shorter horizons - see Figure \ref{fig:best_performers_by_horizon} in the Appendix, which displays performance separately by horizon. We see that the selection method seems to work equally well (or rather, poorly) across all four weeks into the future, albeit with some variations. Moreover, considering the estimation of weights, we are already including seven shorter term forecasts vs. three longer term forecasts, so we would not expect any big changes in weights by further restricting this to only horizons one and two weeks ahead.\footnote{But now that I think about this, larger horizons also give higher scores, argghhhhhh} We thus refrained from additionally implementing this strategy. \medskip \\
%Furthermore, one could also conceive of the idea that it might be possible to ``meta-learn'' about models weaknesses, e.g. downweight models that are known to overshoot following initial rises in incidence. However, this approach would need a lot more data as well as require teams to not update their models, which they presumably would want to do given that they know it has a particular issue...\\
%Or, we could use exponential smoothing to estimate scores
% For example, fpr the discussed case for Germany, we had the idea that maybe the method works better at shorter horizons, which would be good, as the interest is generally greater in shorter term horizons anyway. A boxplot that illustrates this can be found in the appendix. ACTUALLY THIS MIGHT BE HUGE!!!!! COULD ARGUE THAT IT'S NOT ONLY JUST LONG HORIZONS THAT ARE THE CULPRIT.\\
Lastly, we briefly mention an interesting lead that emerges when consulting Table \ref{tab:rel_wis_best_performers}. For the case of $k = 5$, we can see that the weighted median mostly performs better than the unweighted median, suggesting that in very small model sets, it could actually be of benefit to weight models by their recent performance, rather than a source of extra noise. Put differently, while it does not emerge as a dominant strategy to actively $reduce$ the set of models, weighting could be applied in situations where only a small number of models is available to begin with. This could be further investigated via other locations in the European Hub, some of which have very small model bases, in a future analysis. \medskip\\
%In theory, one could furthermore devise several different weighting strategies, such as restricting the maximum weights, . 
%Lastly, we want to mention an interesting, while this was not the direct goal of this analysis, we do observe that the weighted median seems to work better than the unweighted median for smaller set of models. This suggests that weighting could actually be used to mitigate against bad performance of a single forecaster in small model sets, rather than a source of extra noise. This could be applied in situations where only a small model base is available to begin with. \\
%Furthermore, the difference between the unweighted and weighted median for small $k$ suggests that in situations where there is only a very small model base available, (while it might not be beneficial to exclude models), it might be good to weight them. \medskip\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Overall, at least in the sample we are considering, a combination of forecast selection and a weighted median mostly performs okay for forecasting cases but sometimes fails in situations where its component models have drops in performance (mostly by those models overshooting the target), where the method only slowly updates both its component forecasts and their weights. On the one hand, these are the periods that will most drive aggregate scores, and they are also the periods that are of the most interest to decision makers. During these times, it would often be beneficial for the ensemble to draw on its entire model base, to mitigate the effects of those models that overshoot the target. We also note here that if the considered selection procedures actually performed \textit{better} at predicting changes in trends, one might forgive worse performance in other periods, as these moments of increasing case numbers are of particular interest to decision makers. But since the opposite is true and aggregate scores tend to be similar to or worse than the benchmark, with the weighted median additionally showing a tendency for downward bias, we see no reason whatsoever to actually promote its use. An exception could be a case such as the cases series for Poland, where there do seem to exists some models that exhibit better foresight in predicting future case numbers for the method to successfully select and place weight on, but we would still be cautious in recommending such an approach without further investigation into peak behavior in following times. Ultimately, our advice would thus be that decision makers need to consider they respective model set and decide whether such forecasters with good consistent foresight exist.\\ 
Conversely, for forecasting deaths, it seems to be the case that in the European Hub, even when accounting for the fact that models might have higher skill at forecasting some locations more than others, we can't identify component forecasters that perform consistently better.\medskip\\ 
To conclude, we would thus argue that in lieu of the existence of component forecasts that actually perform reliably well, (weighted) selection does not seem to be a viable strategy. Usually, individual model performance is not as consistent as would be needed, again highlighting the benefits of just using an equally weighted median ensemble that includes all models, which after all has the precise benefit of being able to mitigate occasional deteriorations in performance from one or some of its member models and not being reliant on measures of past performance.\\
\newpage
%To conclude, we would thus argue that, at least in the European Hub, it's not inherently Cases vs. Deaths, but simply the presence of good and consistent performers at a given location. For instance, Poland contains models for forecasting cases that exhibit better foresight than others, making the approach more viable.\\
%If asked to generally judge the viability of the (weighted) selection approach with respect to the underlying situation, we would say that our results in conjunction with the results from the U.S. Hub, suggest that the viability of best performers is both a function of the numbers of models to choose from (the added marginal benefit of indiscriminately adding another model to reduce ensemble variability needs to be small) and the existence of some actual consistently good performers. There is also a trade-off between the two: the more consistent the top performers, the smaller the threshold - an example for this is Poland, where $k = 5$ even outperformed $k = 10$. The less consistent, the more models are needed to mitigate any variations in performance. Finally, if model performance is very erratic and past relative good performance is not a reliable predictor for current performance (``models are all the same''), selecting best performers will not work.\\
%Furthermore, we would not suggest it in a situation where the model base is already quite small. \\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Thus, while we initially thought that the (weighted) selection method could fail in the European Hub due to the small . would argue that a larger model base would perhaps be helpful, in conjunction with the findings in the US, what we really need are consistent good performers.
%Discuss why it might have worked well in the U.S., but not here (plus reasonings for why it might work well in the US). However, even when investigating the performance over time, suggesting that individual model performance is not as consistent as would be needed and thereby again highlighting the wonders of just using an equally weighted ensemble, which after all has the precise benefit of being able to mitigate occasional and not being reliant on shaky measures of past performance. \\
%To conclude, we thus argue that for model selection to actually make sense, one either needs to be in a space on the model number line where the added marginal benefit of (or conversely, removing one model does not ).\\ \\
%In conclusion, we thus would not recommend in a setting where the model base is already quite small.\\
%Give some coverage plots.\\
%Lastly, one very hypothetical problem with best performers could also be. Consider the case where all model but one slightly overpredict the target, while the last model substantially underpredicts. That model would receive a low score and would never be included in the best performers set, but would be a valuable addition (at least in a median ensemble). However, since we generally run on the assumption that models are not consistently biased in such a way, we disregard this possibility. \\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Weighting based on model types} \label{sub:weighting_based_on_model_types}
In this section, we discuss our efforts of implementing weighting schemes that are based on model types as discussed previously.\\
As we saw in section \ref{sub:model_types_analysis}, it was not possible to give an overall consistent ranking of model types in the data. One could however then surmise that in specific situations, one general type of modeling philosophy could be better suited than another for giving accurate predictions. We also saw some tentative evidence of this in section \ref{sub:model_types_analysis}: considering Figure \ref{fig:pw_comp_modeltypes_byloc}, we see that in terms of the WIS, it would have for instance been beneficial for most locations to rely more heavily on the statistical models within the set during ``Period 5'', that is, the period that observed the most growth.\\
The idea of selecting component forecasts for an ensemble based on their model type does have some precedence in the literature. In their analysis of the deaths series in the U.S. Hub, \cite{taylor_combining_2021} identified that for states that had relatively low overall mortality compared with others, ensembles that exclusively consisted of compartmental models gave the most accurate forecasts, while the same strategy gave similar performance for states with medium mortality and worsened performance for states with high mortality, compared to an ensemble including all types of models. Apart from this, 
we are unaware of any studies attempting to base ensemble composition or weights on model types.\\
We thought about implementing a similar strategy as the one by \cite{taylor_combining_2021}, but given the small number of locations in our set, which furthermore show similar levels of incidence when accounting for population size, we did not think this to a viable idea. We instead originally considered the idea of categorizing phases of the pandemic (based e.g. on some notion of ``stable'', ``upswing'', ``downswing''), as we also believed that differences between model types would most likely show along these lines (e.g. statistical models might be better at modeling exponential growth of cases) - we however also ultimately found that any such categorization would be arbitrary to a non-negligible degree, since most series exhibited intermittent periods of growth even during the summer months (which did see lower incidence in general).\\
Hence, given our findings that selecting and/or weighting \textit{individual} component forecasts based on recent performance measures does not seem to reliably improve the ensemble, the question arose whether it might be possible to leverage differences in recent performance at the level of model types, rather than at the individual model level. We thus decided to implement a strategy that estimated weights based on model types.\\
Before we give the implementation and our results, we want to discuss some reasons why we thought this could and could not work. As already mentioned, we did observe some tentative evidence that some model types performed better than others during some periods, and we wanted to test whether an automatic weighting scheme might be able to pick up on this. Moreover, grouping models and thereby weighting at the level of model types rather than individual models has the advantage of being able to use all models, even if they newly enter into the Hub and no individual record of performance is available yet. Furthermore, due to models dropping in and out of predicting, there sometimes exist gaps for each model, so we often lack a consistent record of recent performance to base weight estimation on. On the level of model types, we don't have these gaps.\footnote{except for a brief section at the beginning of the study period where we have no statistical models for UK, France and CZ for forecasting cases.} Lastly, this procedure presumably removes some variability from the practice of estimating weights: if a single model has some fluctuations in performance, this will impact a grouped score less than individual scores.\\
However, while model types do share a common approach to modeling, they are of course not a monolith and characteristics other than their categorization within these groups can dominate their performance. The categorization might thus not be meaningful enough. But most of all, this strategy could presumably suffer from the same problem as that which we discussed in the previous section: performance could not be stationary enough to allow good estimation of weights and a weighted ensemble might thus be too slow at adapting to changes in trends. \\
We devised two different and unique methods to test this strategy out: for the first, we decided to use the inverse score weighting approach as used in the previous section, but computed the average score in the respective window at the level of model types rather than single models. From this, we (as previously) calculated a weight for each model type, which we then divided by the number of models available for it and assigned the resulting weight to the respective member models. In other words, this amounts to weighting the groups unequally, but giving each component forecast within each group equal weight. Differently to before, we decided to use an exponential weighting scheme to calculate the recent score of model types, as we hoped that this would let the method adapt faster to changes in performance, while simultaneously still including faraway scores so as not to increase the variance of the estimation too much, albeit to a lesser degree.\\
\begin{table}[t]
\centering
\begin{tabular}{llcccccc}
  \hline
 & Method & Average & Czech R. & Germany & France & U.K. & Poland \\ 
  \hline
\multirow{3}{*}{\rotatebox[origin=c]{90}{Cases}}& Inverse score weighted median & 0.996 & 1.021 & 1.187 & 0.991 & 0.895 & 0.995 \\ 
& QRA & 1.268 & 1.239 & 1.123 & 1.177 & 1.488 & 1.197 \\ 
& QRA - adj. weights & 1.159 & 1.165 & 1.038 & 1.118 & 1.287 & 1.121 \\[0.8em]
\multirow{3}{*}{\rotatebox[origin=c]{90}{Deaths}}& Inverse score weighted median & 1.141 & 1.059 & 1.176 & 0.856 & 1.047 & 1.404 \\ 
& QRA & 1.318 & 1.124 & 1.234 & 1.085 & 1.172 & 1.707 \\ 
& QRA - adj. weights & 1.169 & 0.948 & 1.096 & 1.191 & 1.021 & 1.388 \\ 
   \hline
\end{tabular}
\caption{Table showing the relative performance of the methods compared to the benchmark (an equally weighted median based on the same models) \todos{stratify this by horizon?}}
\label{tab:mt_weights}
\end{table}
%between bias and variance when estimating recent performance of a model: using only a few recent observations to build average performance gives low bias, but also increases variance due to possible random fluctuations in model performance. Exponential weighting of the scoring history gives more weight to recent scores, but also includes faraway scores to a lesser degree so as to reduce the variance of the estimation. Mainly, we did this, while nevertheless not wholly excluding scores that are farer away.\\ 
As a second strategy, we first built an equally weighted median ensembles from each of the model types, thus in effect giving three smaller ensemble models. For these, we then calculated QRA weights as detailed in section XX. We however noticed that the resulting weights could be quite variable and also sometimes tended toward the extreme ends (at certain weeks exclusively giving all weight to one of the ensembles). Furthermore, we thought it wiser to additionally account for the differing availability of models for the respective model types over time amount of models within each group, we additionally decided to implement a simple adjustment strategy that amounts to balancing the weights equally between those that the QRA algorithm calculated and those that are purely based on the size of the respective ensembles.\\
Since we want to focus on the general viability of the approach and in section \ref{sub:model_types_analysis} we mainly considered the three dominant model types within the Hub, we here use the equally weighted median ensemble including only models of those types in the analysis - in practice, this thus excludes the expert judgment based model as well as the two agent-based models for Poland. We do this to see whether a weighting via categorization is \textit{in principle} fruitful or possible and want to remove the confounding factor of ensemble size. At the end of the chapter, we however also compare against the true benchmark for full disclosure. \\ 
The results, again stratified by location and target type, are shown in \ref{tab:mt_weights}
%For this section, we base comparison on the equally weighted median based on the same set of models. We do this to see whether a weighting via categorization is in theory fruitful or possible. The full comparison will be in the overall assessment in subsection ...\\
%To summairse: the hope is that if there are periods of the pandemic where a certain model type has an edge over others, this system will recognize this and weight accordingly. \\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% while \citep{bracher_pre-registered_2021} have identified that following changes in trends, statistical models perform more poorly, likely because they first have to observe the changing trend in the data from which they extrapolate.\\
%As stated before, weighting might not be the best idea. However, in practice there might be correlations of forecast performance - in sofar as this happens at the level of model types, one might be able to exploit that.\\
%Schreib irgendwas darüber warum vielleicht ein Modelltyp einem anderen theoretisch überlegen sein könnte in bestimmten Phasen. Statistical exponential,... This might be hard to implement explicitly, but automatic weighting might work.\\
%To echo this, during our investigation of model types in section ??, we also observed that , although these did not necessarily follow any strict rules as outlined above. Nevertheless, if we for instance regard Figure ??, we can most prominently see for e.g. France that model type performance varies by period, with one type dominating the other at certain times. It would thus have been preferable to use only one type of model. Thus, we want to investigate whether these relative performance differences can be picked up by an automatic weighting scheme. \\
%\cite{taylor_combining_2021} conjecture that during periods of low incidence, mechanistic models should perform better than statistical ones. This is due to the fact that random statistical fluctuations can still occur, but statistical models might, somehow, latch on to these too eagerly and proceed to forecast exponential growth where there is none.\\
%Two problems with inverse score weighting that shrinkage towards equal weights can fix: \\
%- can overweights on e.g. statistical (which could be problematic because past predictive power is of course not a perfect; overshoot after exponential growth)
%- we don't optimize for performance at all. Linear combination of two weights is of course not super flexible, but it gives at least a small dial for actual performance optimization to happen. \\ 
%And anyway, results from previous section did not show robust trends. Instead, the idea is to use recent performance of the model types to weight them differently in an ensemble. As mentioned, due to models dropping in and out of predicting, there are substantial gaps for each model and thus not yielding a consistent record of recent performance that one could use to weight on. On the level of model types, we don't have these gaps. The idea thus is to first build an ensemble for each model type in the data and then to weight these respective ensembles with respect to their recent performance. The hope is that if there are periods of the pandemic where a certain model type has an edge over others, this system will recognize this and weight the models accordingly. \\
%%%%%%%%%%%%%%%%%%%%%%%%
%Semi-rem: Thus, even bad models will provide some benefit\\
\subsection{Overall}
Is mean or median better in period 5? Would expect median to be more robust in times of stark growth.
\section{Contributions}
``We have used the WIS and probabilistic calibration to measure the extent to which forecasts
are consistent with the eventually observed data. These summaries of performance are commonly
used and provide useful insights into forecast performance, but it is worth noting that they do
not necessarily reflect the utility of the forecasts for every particular decision-making context.
Aggregated summaries of performance, such as overall quantile coverage rates could obscure finerscale
details —for instance, a method with good coverage rates on average could have high coverage
at times that are relatively unimportant and low coverage when it matters. Additionally, for some
public health decision-making purposes, one or another aspect of a forecast may be more important;
for example, some users may prioritize accurate assessments about when a new wave may begin,
but other users may find accurate forecasts of peak intensity to be more important. Our evaluation
metrics do not necessarily reflect the particular needs of those specific end users, and it is possible
that different ensemble methods would be more or less appropriate to generate forecasts that serve
different purposes.'' \citep{ray_comparing_2022}.
A lot of work has been done on the US forecast hub. We wanted to investigate whether improvement gains could be made in a setting with a smaller model base and less consistent submissions. While Sherratt showed that weighting did not work, we wanted to know... Ultimately, we came to the same conclusion that has been highlighted before in other studies: past performance is not a reliable predictor for current performance, neither at the individual model level or at the level of model groups/types. \\
While we saw in section \ref{sub:model_types_analysis} that a retrospective analysis showed that there were some systematic differences between model types, when trying to mimic a real-time setting and simply relying on measures of recent past performance on how to weight model types, we saw that it was not possible to consistently make use of this when simulating a real-time setting.  \\
Thus, even though we, contrary to the U.S. Hub approach, addressed both the lack in flexibility and the number of models , thereby ruling these out as the sole causes of the issue.\\\\
Perhaps an overarching meta result from our study is that locations in the European Hub wildly differ and what may . It might sometimes be possible to find methods that seem to work in the aggregate, but that end up showing wildly different results when looking at locations separately. Overall, we would argue that the countries are too heterogeneous to establish a central modeling strategy.\\
The fundamental problem with the best performers approach is thus that it reduces the model set based on past performance, which given the nonstationarity of model performance, is not a reliable predictor for future performance - once a model is in there, it will be given positive weight - if it then performs poorly, this will have a bigger effect on the ensemble than if the ensemble instead drew from its entire model base.\\
Lastly, we again want to caveat that our method of evaluation is very descriptive and frankly too high dimensional. Ideally, one would want to employ a model to consolidate dimensions, but we sadly can't do that. \\
Overall, we think that we have demonstrated that the countries in the set are simply too heterogeneous to warrant indiscriminate statements. Our recommendation would be to always regard the specific situation and wherever resources and or the dominant models aren't available, to rely on the simple median ensemble instead.\\
One could also conceive a strategy where one adaptively changes weights after .\\
\begin{figure}
\centering
\includegraphics[width = 0.95\textwidth]{plot_placeholder/perform_ensemble_ovrtime.png}
\caption{From \cite{ray_ensemble_2020}. Plot different ensemble techniques over time}
\
\end{figure}
\bibliography{references}
\bibliographystyle{plainnat}

\appendix
\begin{figure}
\centering
\includegraphics[width = 0.95\textwidth]{../plots/appendix/best_performers_boxplot2.pdf}
\caption{Best performers, by horizon}
\label{fig:best_performers_by_horizon}
\end{figure}
\end{document}

